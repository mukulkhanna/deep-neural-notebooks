<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Mukul Khanna - Deep Neural Notebooks]]></title><description><![CDATA[Research, Insights & Stories]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Mukul Khanna - Deep Neural Notebooks</title><link>http://localhost:2368/</link></image><generator>Ghost 3.21</generator><lastBuildDate>Fri, 30 Oct 2020 23:33:12 GMT</lastBuildDate><atom:link href="http://localhost:2368/author/mukul/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Deep Neural Notebooks: About the Podcast]]></title><description><![CDATA[Deep Neural Notebooks is a podcast where I like to discuss topics ranging from Deep Learning, Computer Vision and NLP, to Neuroscience and Open Source]]></description><link>http://localhost:2368/deep-neural-notebooks-podcast/</link><guid isPermaLink="false">5f938781f123610eb9e4298b</guid><category><![CDATA[Introduction]]></category><category><![CDATA[Podcast]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Sat, 24 Oct 2020 01:48:03 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-24-at-7.59.26-AM.png" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-24-at-7.59.26-AM-copy.png" class="kg-image" alt="Deep Neural Notebooks: About the Podcast" srcset="http://localhost:2368/content/images/size/w600/2020/10/Screenshot-2020-10-24-at-7.59.26-AM-copy.png 600w, http://localhost:2368/content/images/size/w1000/2020/10/Screenshot-2020-10-24-at-7.59.26-AM-copy.png 1000w, http://localhost:2368/content/images/2020/10/Screenshot-2020-10-24-at-7.59.26-AM-copy.png 1439w" sizes="(min-width: 1200px) 1200px"></figure><img src="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-24-at-7.59.26-AM.png" alt="Deep Neural Notebooks: About the Podcast"><p><strong>Deep Neural Notebooks</strong> is a podcast where I like to discuss topics ranging from Deep Learning, Computer Vision and NLP, to Neuroscience and Open Source Software, through conversations with experts about their thoughts on the state of their specialisations, how things fit into the bigger picture, their journey so far and the road ahead.</p><p>I believe that it is through conversations like these that we can boil down the essence of vast resources of knowledge and expertise into more consumable bits that can enrich our understanding of concepts and technologies that are shaping our world.</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/10/image-7.png" class="kg-image" alt="Deep Neural Notebooks: About the Podcast"></figure><p><strong>Links:</strong> <a href="https://www.youtube.com/channel/UC66w1T4oMv66Jn1LR5CW2yg">Youtube</a> / <a href="https://podcasts.apple.com/us/podcast/deep-neural-notebooks/id1488705711?uo=4">Apple Podcasts</a> / <a href="https://open.spotify.com/show/2eq1jD7V5K19aZUUJnIz5z">Spotify</a> / <a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8xMDZkYzIzOC9wb2RjYXN0L3Jzcw==">Google Podcasts</a> / <a href="https://anchor.fm/deep-neural-notebooks">Anchor.fm</a></p>]]></content:encoded></item><item><title><![CDATA[Practical Natural Language Processing Book Giveaway üìï| NLP, AI in Industry | GPT-3 &¬†more]]></title><description><![CDATA[<p>Here‚Äôs my conversation with authors of the <a href="http://www.practicalnlp.ai/" rel="nofollow noopener noopener noopener"><strong>Practical Natural Language Processing book‚Ää</strong></a>‚Äî‚Ääall about the book, GPT-3 and Machine Learning/AI &amp; NLP in the industry.</p><figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/ptTlH-ma8rg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><h4 id="giveaway-information-">GIVEAWAY INFORMATION:</h4><p>Thanks to O‚ÄôReilly and the authors, we are giving away 5 copies of the Practical Natural Language Processing book.</p><p><strong>Giveaway</strong></p>]]></description><link>http://localhost:2368/pnlp-giveaway/</link><guid isPermaLink="false">5f93bcaff123610eb9e42b34</guid><category><![CDATA[NLP]]></category><category><![CDATA[Machine Learning]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Mon, 11 Nov 2019 05:38:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-24-at-11.10.51-AM.png" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-24-at-11.10.51-AM.png" alt="Practical Natural Language Processing Book Giveaway üìï| NLP, AI in Industry | GPT-3 &¬†more"><p>Here‚Äôs my conversation with authors of the <a href="http://www.practicalnlp.ai/" rel="nofollow noopener noopener noopener"><strong>Practical Natural Language Processing book‚Ää</strong></a>‚Äî‚Ääall about the book, GPT-3 and Machine Learning/AI &amp; NLP in the industry.</p><figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/ptTlH-ma8rg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><h4 id="giveaway-information-">GIVEAWAY INFORMATION:</h4><p>Thanks to O‚ÄôReilly and the authors, we are giving away 5 copies of the Practical Natural Language Processing book.</p><p><strong>Giveaway tweet: </strong><a href="https://twitter.com/mkulkhanna/status/1306233341092360194" rel="noopener">https://twitter.com/mkulkhanna/status/1306233341092360194</a></p><p>To participate in the giveaway, <strong>retweet and comment</strong> about your favourite part of the conversation with the ‚Äú#practicalnlp‚Äù hashtag.</p><h4 id="results">RESULTS</h4><p>The 5 winners will be selected and notified on <strong>October 1, 2020</strong>.</p><p>To be updated about the results, subscribe to the channel and follow me on twitter <a href="http://TWITTER.com/mkulkhanna" rel="noopener">@mkulkhanna</a>.</p><h4 id="free-trial">FREE-TRIAL</h4><p>You can also get a 30-day free trial from the O‚ÄôReilly website by using the promo code PNLP20 or the link below.</p><p>Link: <a href="https://learning.oreilly.com/get-learning/?code=PNLP20" rel="noopener">https://learning.oreilly.com/get-learning/?code=PNLP20</a></p><h4 id="selected-winners">SELECTED WINNERS</h4><ul><li><a href="https://twitter.com/roschler">Robert Oschler</a></li><li><a href="https://twitter.com/deraadt">Music-Health-Tech</a></li><li><a href="https://twitter.com/rajvirdhakhada7">Virendra Dhakhada</a></li><li><a href="https://twitter.com/venksaiyan">Venkat Raman</a></li><li><a href="https://twitter.com/crosstrainmind">Matthew Emerick</a></li></ul>]]></content:encoded></item><item><title><![CDATA[DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë]]></title><description><![CDATA[<p>CVPR 2017, Best Paper Award winner</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt><figcaption>Dense connections</figcaption></figure><blockquote><strong>‚ÄúSimple models and a lot of data trump more elaborate models based on less data. ‚Äú‚Ää‚Äî‚Ää</strong>Peter Norvig</blockquote><h3 id="about-the-paper">About the paper</h3><p>‚Äò<strong>Densely Connected Convolutional Networks</strong>‚Äô received the <strong>Best Paper Award</strong> at the IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>) <strong>2017</strong>. The</p>]]></description><link>http://localhost:2368/densenet-densely-connected-convolutional-networks/</link><guid isPermaLink="false">5f93aa97f123610eb9e42a54</guid><category><![CDATA[8k+ views]]></category><category><![CDATA[Paper Review]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Sun, 10 Nov 2019 04:23:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/1_TeHVqikNc68QC98Vm9M98Q.gif" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2020/10/1_TeHVqikNc68QC98Vm9M98Q.gif" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><p>CVPR 2017, Best Paper Award winner</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption>Dense connections</figcaption></figure><blockquote><strong>‚ÄúSimple models and a lot of data trump more elaborate models based on less data. ‚Äú‚Ää‚Äî‚Ää</strong>Peter Norvig</blockquote><h3 id="about-the-paper">About the paper</h3><p>‚Äò<strong>Densely Connected Convolutional Networks</strong>‚Äô received the <strong>Best Paper Award</strong> at the IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>) <strong>2017</strong>. The paper can be read <a href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener">here</a>.</p><p>The primary author, <a href="http://www.gaohuang.net/" rel="noopener"><strong>Gao Huang</strong></a> has been a Postdoctoral Fellow at Cornell University and is currently working at Tsinghua University as an Assistant Professor. His research focuses on deep learning for computer vision.</p><h3 id="how-i-came-across-the-paper">How I came across the paper?</h3><p>I came across this paper while researching for neural network implementations that were focused on improving image quality (in terms of resolution or dynamic range) by reconstruction. Although this paper demonstrates the prowess of the architecture in image classification, the idea of dense connections has inspired optimisations in many other deep learning domains like image super-resolution, image segmentation, medical diagnosis etc.</p><h3 id="key-contributions-of-the-densenet-architecture">Key contributions of the DenseNet architecture</h3><ul><li>Alleviates <strong>vanishing gradient problem</strong></li><li>Stronger <strong>feature propagation</strong></li><li><strong>Feature reuse</strong></li><li><strong>Reduced parameter</strong> count</li></ul><blockquote><strong>Before you read: </strong><br>Understanding this post requires a basic understanding of deep learning concepts.</blockquote><h3 id="paper-review">Paper review</h3><p>The paper starts with talking about the <strong>vanishing gradient problem‚Ää‚Äî‚Ää</strong>about how, as networks get deeper, gradients aren‚Äôt back-propagated sufficiently to the initial layers of the network. The gradients keep getting smaller as they move backwards into the network and as a result, the initial layers lose their capacity to learn the basic low-level features.</p><p>Several architectures have been developed to solve this problem. These include‚Ää‚Äî‚ÄäResNets, Highway Networks, Fractal Nets, Stochastic depth networks.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*MwxLN83uT8bgd4nGiR6JZQ.jpeg" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"></figure><p>Regardless of the architectural designs of these networks, they all try to create channels for information to flow between the initial layers and the final layers. DenseNets, with the same objective, create paths between the layers of the network.</p><h3 id="related-works">Related works</h3><ul><li>Highway networks (one of the first attempts at making training easy for deeper models)</li><li>ResNet (Bypassing connections by summation using identity mappings)</li><li>Stochastic depth (dropping layers randomly during training)</li><li>GoogLeNet (inception module‚Ää‚Äî‚Ääincreasing network width)</li><li>FractalNet</li><li>Network in Network (NIN)</li><li>Deeply Supervised Network (DSN)</li><li>Ladder Networks</li><li>Deeply-Fused Nets (DFNs)</li></ul><h3 id="dense-connections">Dense connections</h3><p>Following the feed-forward nature of the network, each layer in a dense block receives feature maps from all the preceding layers, and passes its output to all subsequent layers. Feature maps received from other layers are fused through <strong>concatenation</strong>, and not through summation (like in ResNets).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*StSq7XyHcuxUOfWuuALkdw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption>Concatenation of feature&nbsp;maps</figcaption></figure><p>These connections form a dense circuit of pathways that allow <strong>better gradient-flow</strong>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption>Dense connections</figcaption></figure><blockquote>Each layer has direct access to the gradients of the loss function and the original input signal.</blockquote><p>Because of these dense connections, the model requires fewer layers, as there is no need to learn redundant feature maps, allowing the <strong>collective knowledge </strong>(features learnt collectively by the network) to be reused. The proposed architecture has narrow layers, which provide state-of-the-art results for as low as 12 channel feature maps. Fewer and narrower layers means that the model has <strong>fewer parameters</strong> to learn, making them easier to train. The authors also talk about the importance of variation in input of layers as a result of concatenated feature maps, which prevents the model from over-fitting the training data.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*04TJTANujOsauo3foe0zbw.jpeg" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"></figure><p>Many variants of the DenseNet model have been presented in the paper. I have opted to explain the concepts with their standard network (DenseNet-121).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*rkra9kVPl754-vjRGsOqvw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption>Some of the variants of the DenseNet architecture</figcaption></figure><h3 id="composite-function">Composite function</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*qA2rLVBRB-wZoI3nAEMdkA.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption>Composite function</figcaption></figure><p>*Each CONV block in the network representations in the paper (and in the blog) corresponds to an operation of ‚Äî</p><p><strong>BatchNorm‚ÜíReLU‚ÜíConv*</strong></p><h3 id="dense-block">Dense block</h3><p>The concept of dense connections has been portrayed in dense blocks. A dense block comprises <em>n </em>dense layers. These dense layers are connected using a dense circuitry such that each dense layer receives feature maps from all preceding layers and passes it‚Äôs feature maps to all subsequent layers. The dimensions of the features (width, height) stay the same in a dense block.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*ZfrliiHwn_L4kKcO61Oxgw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption><strong class="markup--strong markup--figure-strong">Dense block (DB) with six Dense Layers&nbsp;(DL)</strong></figcaption></figure><h3 id="dense-layer">Dense layer</h3><p>Each dense-layer consists of 2 convolutional operations -</p><ul><li><strong>1 X 1 CONV </strong>(conventional conv operation for extracting features)</li><li><strong>3 X 3 CONV </strong>(bringing down the feature depth/channel count)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*cRXqccOxYkZbWpfmyXzjog.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption><strong class="markup--strong markup--figure-strong">Dense layer of&nbsp;DB-1</strong></figcaption></figure><p>The DenseNet-121 comprises of 6 such dense layers in a dense block. The depth of the output of each dense-layer is equal to the growth rate of the dense block.</p><h3 id="growth-rate-k-">Growth rate (k)</h3><p>This is a term you‚Äôll come across a lot in the paper. It is basically the number of channels output by a dense-layer (<em>1x1 conv ‚Üí 3x3 conv</em>). The authors have used a value of <em>k = 32</em> for the experiments. This means that the number of features received by a dense layer ( <em>l </em>) from it‚Äôs preceding dense layer ( <em>l-1</em> ) is 32. This is referred to as the growth rate because after each layer, 32 channel features are concatenated and fed as input to the next layer.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*NIQenf9KTillNhYfuySfMw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption>Dense block with channel count (C) of features entering and exiting the&nbsp;layers</figcaption></figure><h3 id="transition-layer">Transition layer</h3><p>At the end of each dense block, the number of feature-maps accumulates to a value of‚Ää‚Äî‚Ää<em>input features + (number of dense layers x growth rate). </em>So for 64 channel features entering a dense block of 6 dense-layers of growth rate 32, the number of channels accumulated at the end of the block will be‚Ää‚Äî‚Ää<br>64 + (6 x 32) = 256. To bring down this channel count, a <strong>transition layer </strong>(or block) is added between two dense blocks. The transition layer consists of -</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*LoFEV57u5kCjsqZRX7EAKw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption><strong class="markup--strong markup--figure-strong">Transition layer/block</strong></figcaption></figure><ul><li><strong>1 X 1 CONV</strong> operation</li><li><strong>2 X 2 AVG POOL </strong>operation</li></ul><p>The <strong>1 X 1 CONV </strong>operation reduces the channel count to half.<br> The <strong>2 X 2 AVG POOL</strong> layer is responsible for downsampling the features in terms of the width and height.</p><h3 id="full-network">Full network</h3><p>As can be seen in the diagram below, the authors have chosen different number of dense layers for each of the three dense block.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*CE11_lfEz00aoOjLiw5sdw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption><strong class="markup--strong markup--figure-strong">Full DenseNet architecture</strong></figcaption></figure><h3 id="comparison-with-densenet">Comparison with DenseNet</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*cXAoIC_ig5R8nE4hKj2OeQ.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption>ResNet DenseNet comparison</figcaption></figure><p>We can see that even with a reduced parameter count, the DenseNet model has a significantly lower validation error for the ResNet model with the same number of parameters. These experiments were carried out on both the models with hyper-parameters that suited ResNet better. The authors claim that DenseNet would perform better after extensive hyper-parameter searches.</p><blockquote>DenseNet-201 with 20M parameters model yields similar validation error as a 101-layer ResNet with more than 40M parameters.</blockquote><h3 id="inspecting-the-code">Inspecting the code</h3><p>I believe that going through the code makes it easier to understand the implementations of such architectures. Research papers (in the context of Deep Learning) can be difficult to understand because they are more about what drives the design decisions of a neural network. Inspecting the code (usually the network/model code) can reduce this complexity because sometimes it‚Äôs just the implementation that we are interested in. Some people prefer first seeing the implementation and then trying to figure out the reasoning behind the design decisions of the network. Regardless, reading the code, before or after, always helps.</p><p>The code of the DenseNet implementation can be found <a href="https://github.com/liuzhuang13/DenseNet" rel="noopener">here</a>. Since I am more comfortable with PyTorch, I‚Äôll try to explain the PyTorch implementation of the model which can be found <a href="https://github.com/gpleiss/efficient_densenet_pytorch" rel="noopener">here</a>. The most important file would be <strong>models/densenet.py</strong>, that hold the network architecture for DenseNet.</p><p>The code has been divided into these classes where each type of block is represented by a class.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*RgsliXFElnLQ5uh5ZCDBAg.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption>Class hierarchy in&nbsp;code</figcaption></figure><h3 id="dense-layer-1">Dense layer</h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/1200979cc91e54f2e638872fd4680560.js"></script>
<!--kg-card-end: html--><p>The<strong> _DenseLayer </strong>class can be used to initialise the constituent operations of a dense layer ‚Äî</p><p><strong>BatchNorm ‚Üí ReLU ‚Üí Conv (1X1) ‚Üí BatchNom ‚Üí ReLU ‚Üí Conv (3X3)</strong></p><p>The <strong>_bn_function_factory()</strong> function is responsible for concatenating the output of the previous layers to the current layer.</p><h3 id="denseblock"><strong>DenseBlock</strong></h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/0783bc22b0b8f826ef92e1e7455c3075.js"></script><!--kg-card-end: html--><p>The _<strong>DenseBlock </strong>class houses a certain number of <strong>_DenseLayer</strong>s (<em>num_layers</em>).<br>This class is initialised from the <strong>DenseNet </strong>class depending on the number of dense blocks used in the network.</p><h3 id="transition-block">Transition Block</h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/86feacfbdbbc4ad841adac0e00e4de75.js"></script><!--kg-card-end: html--><h3 id="densenet"><strong>DenseNet</strong></h3><p>Since this part of the code is a little too big to fit in this blog, I‚Äôll just be using a part of the code that should help in getting the gist of the network.</p><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/2238b561aca1c29878310fe322f1ba54.js"></script><!--kg-card-end: html--><p>I found this image online and it has helped me understand the network better.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*vIZhPImFr9Gjpx6ZB7IOJg.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"><figcaption><a href="https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a" data-href="https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Source</a></figcaption></figure><h3 id="other-works-inspired-from-this-paper">Other works inspired from this paper</h3><ul><li><a href="https://arxiv.org/abs/1802.08797" rel="noopener">Residual Dense Network for Image Super-Resolution</a> (2018)</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*6NS5NPZoU3iQXIJOu6jruQ.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"></figure><ul><li><a href="http://www.statnlp.org/research/lg/zhijiang_zhangyan19tacl-gcn.pdf" rel="noopener">Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning</a> (2019)</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*pa45Gjp8H1D9NnXJE6Vypw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) üìë"></figure><h3 id="conclusion">Conclusion</h3><p>DenseNet is a network that portrays the importance of having dense connections in a network using dense blocks. This helps in feature-reuse, better gradient flow, reduced parameter count and better transmission of features across the network. Such an implementation can help in training deeper networks using less computational resources and with better results.<br></p>]]></content:encoded></item><item><title><![CDATA[Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è]]></title><description><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/1*5zndHrmiMesUwi3U5zU0sA.jpeg" class="kg-image" alt></figure><p>This summer, my proposal got selected by <a href="https://www.openrobotics.org" rel="noopener">Open Source Robotics Foundation</a> for Google Summer of Code 2019. It was an awesome learning experience!</p><h3 id="open-source-robotics-foundation">Open Source Robotics Foundation</h3><p>Open Source Robotics Foundation (OSRF), or Open Robotics is an independent non-profit organisation that works on open software and hardware for use in</p>]]></description><link>http://localhost:2368/google-summer-of-code-2019-is-a-wrap/</link><guid isPermaLink="false">5f93ad97f123610eb9e42a7e</guid><category><![CDATA[GSOC]]></category><category><![CDATA[robotics]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Tue, 29 Oct 2019 04:30:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-24-at-10.04.23-AM.png" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/1*5zndHrmiMesUwi3U5zU0sA.jpeg" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"></figure><img src="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-24-at-10.04.23-AM.png" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><p>This summer, my proposal got selected by <a href="https://www.openrobotics.org" rel="noopener">Open Source Robotics Foundation</a> for Google Summer of Code 2019. It was an awesome learning experience!</p><h3 id="open-source-robotics-foundation">Open Source Robotics Foundation</h3><p>Open Source Robotics Foundation (OSRF), or Open Robotics is an independent non-profit organisation that works on open software and hardware for use in robotics. Their two open-source software projects‚Ää‚Äî‚Ää<a href="http://ros.org/" rel="noopener">ROS</a> and <a href="http://gazebosim.org" rel="noopener">Gazebo</a> are the most commonly adopted frameworks for developing robots and their simulations. ROS and Gazebo have done wonders for the robotics community, driving academic research and product development in robotics all over the world. Open Robotics offers consultance, R&amp;D help, open source software services and custom engineering for robotics to industry and government.</p><h3 id="my-project">My project</h3><p>My GSoC proposal was for the <strong>Gazebo Documentation Index </strong>project. It was mentored by the best, <a href="https://twitter.com/_jrivero_?lang=en" rel="noopener">Jose Luis Rivero</a>, who is a Software Engineer at OSRF. My proposal can be accessed <a href="https://summerofcode.withgoogle.com/serve/6486525154426880/" rel="noopener">here</a>. The documentation index that I developed can be accessed <a href="https://osrf.github.io/gazebo-doc-index/" rel="noopener">here</a> and the corresponding Github repo can be found <a href="https://github.com/osrf/gazebo-doc-index" rel="noopener">here</a>.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*oaBA-IXejsAtwl1W.jpg" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"></figure><p><a href="http://gazebosim.org" rel="noopener"><strong>Gazebo</strong></a> is an open source software built and maintained by OSRF to accelerate research and development using robotic simulations that make it easy to test and validate algorithmic implementations in real-time on robots in simulated environments, without having to deploy actual physical vehicles every time. This helps in reducing costs, resources and time.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*JGB4YJOsl8GWm98q.gif" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><figcaption>ATLAS walking in Gazebo in real&nbsp;time</figcaption></figure><p>At present, the learning resources for Gazebo are distributed over the internet‚Ää‚Äî‚Ääthe official documentation, the Q&amp;A website and ROS-Answers. Some noteworthy help can also be found through examples and explanations in the comments of Bitbucket issues (Gazebo‚Äôs repo) and Gazebo‚Äôs API. There are also third-party sources that provide video tutorials and blog posts that are helpful in learning Gazebo. All of this information is distributed across the internet with some links joining each other.</p><p>It can be a bit overwhelming to keep track of one‚Äôs learning when the content is distributed as in this case.</p><p>The aim of this project was to bring all the learning material under one webpage in the form of a documentation index that contains links to the content where the respective information is hosted.</p><p>A documentation index is a platform where links to relevant learning resources for a software system are indexed to allow users to find any help at one place.</p><p><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Index" rel="noopener">HTML</a>, <a href="https://www.intel.com/content/www/us/en/programmable/documentation/lit-index.html" rel="noopener">Intel</a> and <a href="https://www.oracle.com/technetwork/indexes/documentation/index-100966.html" rel="noopener">Oracle</a>, among many others have such doc indexes in place.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*C8qPqdXlCZk-yUSl6zOc4A.png" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><figcaption>HTML‚Äôs documentation index</figcaption></figure><p>A documentation index is a neat way to accommodate links to all the relevant learning content into one webpage. It is very convenient since almost any help is just a page quicksearch away. The user can think of related keywords or categories and then look through the index to find the relevant information. Such a platform can act as a one-stop place to get all relevant information about Gazebo.</p><ol><li>Compilation of best learning resources from across the internet, including tutorials, third-party blog posts, Bitbucket issues‚Äô comments and the Gazebo answers website.</li><li>All relevant content under one roof.</li></ol><p>My GSoC project was to build such a documentation index for Gazebo.</p><h3 id="proposed-timeline">Proposed timeline</h3><p>The following is the timeline I presented in my proposal -</p><ol><li>Discussion with the mentees about the frontend and backend framework to be used along with the data format in which the index links will be stored.</li><li>Accumulating the data for the index‚Äôs links of where the relevant information is hosted and finalising the structuring of data across the chosen data format files.</li><li>Making index data files available for collaborative maintenance along with rules and recommendations for any contributions to the documentation index in the form of pull requests.</li><li>Simultaneously, developing a prototype that can work with limited number of links.</li><li>Writing test cases to validate the webpage requirements.</li><li>A demo to the mentees and other relevant people in the organisation followed by discussion about areas of improvement in terms of performance and design.</li><li>Working on final version and completion within a stipulated time period.</li><li>After a demo to the mentees, if the implementation is successful, discussion on build and deployment pipelines.</li><li>CI (Continuous Integration) can be set up with Travis CI for automated testing and code building.</li><li>After all the aforementioned requirements are satisfied, the website can be put into production.</li></ol><h3 id="getting-started">Getting started</h3><p>As a part of my proposal for GSoC, I was asked to complete and submit a task that can speak for my experience with web development and a basic understanding of Gazebo. Creating a minimal stub of the doc index website was one of these tasks. My mentor had prepared a basic design for a couple of webpages that I was supposed to emulate.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*RvTYuC_JMAZ1hfxdRXQNiQ.png" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><figcaption>Minimal stub</figcaption></figure><h3 id="choosing-the-stack">Choosing the stack</h3><p>It was during developing these pages that I started thinking about how I could implement the documentation index in a way that it is easy to build, easy to maintain (adding, removing and updating index entries by open-source collaboration), light-weight and fast.</p><p>My preferred tool-kit for building websites was a Vuejs frontend + a Python (Flask) or Nodejs (Expressjs) backend + Heroku for serving a PostgresQL DB and for deployment.</p><p>But this had to be approached differently.</p><p>For example, the doc index would have had been difficult to maintain if the index entries were going to be stored in a conventional database, requiring a lot of pipeline setup, sync issues and making it difficult for any Gazebo developer who wishes to contribute because of requirement of pre-requisite knowledge of the stack.</p><p>Also, I realised that the website would only require two (or three) major template pages, so going for a comprehensive front-end framework would have been overkill.</p><p>I was looking for a more integrated approach (Oh, Steve Jobs would have been so happy to hear that), that would be easier to maintain.</p><p><a href="https://jekyllrb.com" rel="noopener">Jekyll</a> had everything I was looking for. With it‚Äôs Static Site Rendering (SSR), templating engines and easy data management using YAML-based frontmatter in Markdown files, Jekyll was the perfect choicde.</p><p>The best thing about Jekyll was how the data of the doc index could reside in these adorable Markdown files that were easy to maintain and collaborate on using Github PRs. This helped in eliminating the requirement of a backend server and a database. Because of Github Pages support for Jekyll, hosting also wasn‚Äôt an issue anymore.</p><p>I developed the prototype using Jekyll and after the project started, I discussed the advantages of using Jekyll with my mentor, who agreed that this was the way to go. Some important index entries, that my mentor had listed, were added to the website.</p><h3 id="structuring-the-doc-index">Structuring the doc index</h3><p>The next step was to decide how to structure the doc index in a way that it was easy to scale and again, easy to maintain. We came up with a structure that is two levels deep, classified into categories and subcategories. The first level of abstraction is the category level and the second level is the subcategory level.</p><p><code>Category &gt;&gt; Subcategory &gt;&gt; Index entries</code></p><ul><li>Each category comprises of subcategories and each subcategory comprises of the corresponding index items.</li><li>Each category and index item also contain a brief description of the same.</li><li>More important index items can be shown starred.</li></ul><p>This hierarchical classification, as shown below has complied well with the requirements of the project.</p><pre><code>- category 1    - subcategory 1        - item 1         - item 2</code></pre><pre><code>- subcategory 2        - item 1         - item 2</code></pre><pre><code>- category 2 - category 3</code></pre><h4 id="all-links-page">All links page</h4><p>To allow for the user to be able to find all relevant content on one webpage, an ‚ÄòAll links‚Äô page was also added. This can help users in finding any help using a browser quicksearch (Ctrl + F).</p><h3 id="testing">Testing</h3><p>Test cases had to be written to automate the procedure of verifying that everything worked fine after any update to the code (commits and PRs). The two primary things to test here are ‚Äî</p><ul><li>Integrity of index data structure</li><li>Validity of external links in index items</li></ul><p>For the first part, I wrote a spec <a href="https://github.com/osrf/gazebo-doc-index/blob/5cd394bffd5ba35a61cf72512decc4cb0c5b1390/spec/index_integrity_spec.rb" rel="noopener">file</a> in Ruby that parses all the data files and expects the structure of the index data to be maintained. For testing validity of external links, we used the very common <a href="https://github.com/gjtorikian/html-proofer" rel="noopener">html-proofer</a> library that takes care of that.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*y78zn4NyN0E3KMpV.png" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><figcaption>Index integrity test</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*K1xbY4qY4KKhLkth.png" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><figcaption>HTMLproofer external link validity&nbsp;test</figcaption></figure><p>These test cases have been automated using Travis CI, that runs these tests for us every time there is a commit or a pull-request in the repo.</p><h3 id="user-interface">User interface</h3><p>For the user interface of the website, I made improvements to the minmal stub created by my mentor, trying to make it clean and minimal so that the prime focus was on the doc index entries.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*FZ3oCXILxTi7b4ZNI7MiFA.png" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><figcaption>Home page</figcaption></figure><p>For ease of collaboration, a ‚ÄòSuggest edits‚Äô button was added that directly takes you to the Github editor using which any changes can be made to the index, and a PR can be opened.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*CO5Oe59bZf6OlfZOMcnsCw.png" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><figcaption>Category page</figcaption></figure><h3 id="alpha-launch">Alpha launch</h3><p>With this, we were able to launch the <strong>alpha version </strong>of the Gazebo Documentation Index, inviting developers and users to contribute to the growth of the platform. The alpha release announcement can be accessed <a href="https://community.gazebosim.org/t/gazebo-documentation-index-alpha-release/384" rel="noopener">here</a>.</p><h3 id="what-next">What next</h3><p>This provided us a decent setup to expand on and start finessing the doc index in terms of the data that it provides‚Ää‚Äî‚Ääthe amount of documentation that it covers.</p><p>Other than suggestions from the community about the items that can be added to the doc index, we decided to develop a suggestions-tool‚Ää‚Äî‚Ääan application that can suggest index entries by scraping the issues on Gazebo‚Äôs Bitbucket repository. There is a lot of information in the issues and their comments that is not otherwise documented. These issues can also be a reminder of relevant topics that people are facing difficulties with.</p><p>The suggestions-tool can be accessed <a href="https://gazebo-index.herokuapp.com/" rel="noopener">here</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*Mo3Utkhwrx2QuHW53wAS_g.png" class="kg-image" alt="Google Summer of Code 2019 is a wrap! ‚òÄÔ∏è"><figcaption>suggestions-tool</figcaption></figure><p>The suggestions-tool scrapes content from Bitbucket issues, performs NLP based keyword extraction and provides a list of keywords pertaining to each issue. These keywords help in getting a gist of the issue‚Äôs content. This information can be used to know whether the corresponding topics have been covered in the doc index. If yes, then these can be marked in the suggestions-tool to keep a track of the issues that have already been covered in the doc index.</p><p>After a careful evaluation of the requirements, the app was developed using Vuejs + Flask + PostgresQL (hosted on Heroku) + Heroku.</p><h3 id="future-work">Future work</h3><p>I had a great time working on this project. I will continue to work on this project at least until it gains popularity and reaches the self-sufficient state. The next step is to get more information about this out to the users, and to get people to refer to this in answers to questions on the community page or the Bitbucket issues page.</p><p>These are the very beginning days of the Documentation Index and it has serious potential to become an important documentation resource that brings together all the relevant learning material in one place,in an organised fashion, helping beginners and professionals save time and resources and find the right help immediately.</p>]]></content:encoded></item><item><title><![CDATA[HDR Imaging: What is an HDR image anyway? üì∑]]></title><description><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/0*E1pzpUrIPoMfaWNg.jpg" class="kg-image" alt></figure><p>We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn‚Äôt turn out well. The image comes out to be either too dark or too bright depending on the focus. Let‚Äôs try to understand why this happens and how this</p>]]></description><link>http://localhost:2368/what-is-an-hdr-image-anyway/</link><guid isPermaLink="false">5f93a47ef123610eb9e42a04</guid><category><![CDATA[4k+ views]]></category><category><![CDATA[HDR]]></category><category><![CDATA[Computer Vision]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Thu, 10 Oct 2019 03:56:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/0*E1pzpUrIPoMfaWNg.jpg" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><img src="https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="HDR Imaging: What is an HDR image anyway? üì∑"><p>We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn‚Äôt turn out well. The image comes out to be either too dark or too bright depending on the focus. Let‚Äôs try to understand why this happens and how this can be solved.</p><p>There are a lot of key concepts that revolve around the study of HDR images -</p><ul><li><strong>Dynamic range</strong></li><li><strong>Image exposure</strong></li><li><strong>Shutter speed, Aperture, ISO</strong></li><li><strong>Image bracketing</strong></li><li><strong>Merging LDR images</strong></li><li><strong>Image encoding</strong></li><li><strong>Camera response function</strong></li><li><strong>Linearisation</strong></li><li><strong>Gamma correction</strong></li><li><strong>Tonemapping</strong></li><li><strong>Visualising HDR images</strong></li></ul><h3 id="dynamic-range">Dynamic Range</h3><p>Dynamic range of a scene refers to the range of light intensity that encompasses a scene. It can also be defined as the ratio of light (maximum measurable brightness) to dark (minimum measurable brightness) in an image.</p><p>To get some context of the how brightness can be quantised, the range of light intensity is 0 to infinity, with zero being the darkest and infinity being the brightest source there is (‚òÄÔ∏è) .</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*YX1gBta3mldqWhvBMUmqsA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Luminance value comparison (<a href="https://www.hdrsoft.com/resources/dri.html" rel="noopener">Source</a>)</figcaption></figure><p>No camera is able to capture this complete uncapped range of illuminance in a scene. Therefore, images turn out to be either too bright (overexposed) or too dark (underexposed). These images are called <em>Low Dynamic Range</em> (LDR) images . For images that turn out too bright, it is only the brighter subrange (of the infinite range) that the camera is able to capture, and correspondingly for the darker images, only the lower subrange is captured.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*CzMITGf7VZ6ly2wLKz_d1g.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption><strong>True image</strong> (Left), <strong>Overexposed image</strong> (Center), <strong>Underexposed image</strong> (Right)</figcaption></figure><h3 id="image-exposure">Image Exposure</h3><p>The amount of light entering the camera (and thus, the image) is called the exposure. The exposure of the image can be controlled by three settings of a camera‚Ää‚Äî‚Ääthe aperture, shutter speed and ISO.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*mQ_fmmAzRkmKX63SrfCV8A.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><p><strong>Aperture:</strong> The area of the camera lens through which the light can come in.</p><p><strong>Shutter speed:</strong> The speed with which the shutter of the camera closes. As the shutter speed increases, the amount of light entering the camera decreases, and vice versa. It also improves the sharpness of an image.</p><p><strong>ISO: </strong>Sensitivity of the camera sensor to incoming light.</p><p>I found this nice analogy <a href="https://www.cambridgeincolour.com/tutorials/camera-exposure.htm" rel="noopener">here</a>, between the camera settings and a bucket left out in the rain.</p><blockquote>In photography, the exposure settings of aperture, shutter speed and ISO speed are analogous to the width, time and quantity in the discussion above. Furthermore, just as the rate of rainfall was beyond your control above, so too is natural light for a photographer.</blockquote><p>Back to <em>dynamic range</em>. A single image captured from the camera can not contain the wide range of light intensity.</p><p>This problem can be solved by merging images captured at multiple exposure values. How this helps is that the overexposed images work well for the darker regions in the image, and the underexposed images are able to tone down the intensity in the extra-bright regions. Different regions of the image are captured better at different exposure values. Therefore, the idea is to merge these set of images and to recover an image with a <em>high dynamic range</em> (HDR).</p><h3 id="image-bracketing">Image bracketing</h3><p>Bracketing refers to capturing multiple images of the same scene with different camera settings. It is usually done automatically by the camera. What happens when you use the HDR feature on your smart phone is that the phone captures 3 (usually) images at three different exposure times (or exposure values) in quick succession. The lower the exposure time, the lesser the amount of light that gets in. These three images are merged by the camera software and are saved as a single image, in a way that the best portions of each image make it to the final image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*gkqV3yHAhfQXEYGY.jpg" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>A collage of 5 bracketed images that I found on the <a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiR0PPA4JHjAhVQb30KHdH6DaUQjhx6BAgBEAM&amp;url=https%3A%2F%2Fwww.highdynamicranger.com%2Fhow-to-use-auto-exposure-bracketing-for-hdr%2F&amp;psig=AOvVaw0b8jjGtlbQ010BQW1YHPIQ&amp;ust=1562002928887447" rel="noopener">internet</a>.</figcaption></figure><p>The funny thing here is that the image that is saved on your phone after the merging happens, is still not (technically) an HDR image. This is where the image encodings come into the picture (and also tonemapping‚Ää‚Äî‚Ääwhich we‚Äôll discuss later).</p><h3 id="image-encoding">Image encoding</h3><p>Commonly, the images that we see on our phones and computers, are 8-bit (per channel) encoded RGB images. Each pixel‚Äôs value is stored using 24-bit representations, 8-bit for each channel (R, G, B). Each channel of a pixel has a range of 0‚Äì255 intensity values.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*KyT2vpk6-OCt_Nu9SXCNyw.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Example of 24-bit (3x8-bit) encoding for an RGB pixel</figcaption></figure><p>The problem with this encoding that it is not capable of containing the large dynamic range of natural scenes. It only allows a range of 0‚Äì255 (only integers) for accommodating the intensity range, which is not sufficient.</p><p>To solve this problem, HDR images are encoded using 32-bit floating point numbers, for each channel. This allows us to capture the wide uncapped range of HDR images. There are various formats for writing HDR images, the most common being .hdr and .exr. All HDR images are 32-bit encodings but not all 32-bit images can be HDR images.</p><h3 id="camera-response-function-crf-">Camera Response Function (CRF)</h3><p>CRF is a function that shows the relationship between the actual scene irradiance and the digital brightness values in the image. It is also called as the Opto-electrical transfer function. Camera companies don‚Äôt provide their CRFs and consider it as proprietary information.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*XA43PkIl_VE44Asilb5z2w.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Camera response function example</figcaption></figure><p>In an ideal world, the CRF should have a linear graph‚Ää‚Äî‚Äämeaning, the brightness value of the pixels in the image should be directly proportional to the actual irradiance in the scene. This is true for HDR images, but not for the usual images where the brightness values are altered to be able to contain them in a finite range. The more important reason for conventional images being de-linearised depends on how display devices work.</p><p>Back in the time of CRT (Cathode Ray Tube) displays, electrons were fired on a phosphor surface. The phosphor screen is known to emit photons upon being hit by accelerated electrons. However the brightness of the display didn‚Äôt vary linearly with the strength of the electron beam. This problem has been solved by modifying the incoming image/video source signals non-linearly in the direction opposite to the display‚Äôs non-linearity. By doing this, we can get a fair linear estimate of the natural scene brightness.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*UW_uGegQxkl17PlHU0f2WA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><p>This de-linearisation of the source allows to compensate for a non-linear display. Display technologies have advanced but the non-linearity still exists in most devices. This <em>de-linearisation</em> is known as gamma correction.</p><blockquote>Gamma corrected image = image ^ Œ≥</blockquote><p>If the input image is x, then what a display device of gamma=1.2 shows is x^(1.2). Therefore, the input image is encoded as x^(1/1.2) so that the monitor converts it to x^((1/1.2) x 1.2) which is equal to x, the original image captured by the camera.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*4k6-goBZ-RVLWjd9psEXAA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><p>For most displays these days, images have to be encoded by a gamma value of 0.45 (1/2.2) because of a gamma decoding of 2.2 by the displays.</p><p>Gamma encoding is performed over a range of [0,1]. So images first have to be normalised by dividing by 255 and then again multiplying by 255 after the gamma operation. Powers of greater than 1 yield darker images whereas powers of less than 1 yield brighter image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*grxUUm_ZH2CmaLs-ynH0EA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Comparison between gamma encoded images</figcaption></figure><p>HDR photography (or any photography) is quite complex in a way that we need to think of three important aspects ‚Äî</p><ul><li>How the actual scene is (ground truth / uncapped dynamic range)</li><li>How the camera captures (bracketing and then merging)</li><li>How it is displayed (tonemapping)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*wNoLL8Iz5IIR-pEb.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption><a href="https://www.cambridgeincolour.com/tutorials/dynamic-range.htm" data-href="https://www.cambridgeincolour.com/tutorials/dynamic-range.htm" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Source</a></figcaption></figure><p>In the context of HDR imaging, we have discussed the first two points. Now let‚Äôs look at how HDR images can be displayed.</p><h3 id="tonemapping">Tonemapping</h3><p>Most off the shelf display devices are incapable of delivering the wide uncapped range of HDR images. They expect the input source to be in the three-channel 24-bit (3x8) RGB format. Due to this reason, the wide dynamic range needs to be toned down to be able to accommodate it in the 0‚Äì255 range of RGB format. This can be done in several ways, some of which are-</p><ul><li>Conventional linear normalisation: This is the most basic way of bringing down the wide range of an HDR image.</li></ul><blockquote>tonemapped image = (img‚Ää‚Äî‚Ääimg.min()/img.max()‚Ää‚Äî‚Ääimg.min()) x 255</blockquote><ul><li>Reinhard tonemapping: This is one of the most commonly used tonemapping algorithm that was shared in this <a href="http://www.cmap.polytechnique.fr/~peyre/cours/x2005signal/hdr_photographic.pdf" rel="noopener">paper</a>.</li></ul><blockquote>tonemapped image = img/(1+img) x 255</blockquote><ul><li>Drago tonemapping: This tonemapper is a perception-based one that compresses the dynamic range using logarithmic functions ‚Äúcomputed using different bases from the scene content‚Äù. The paper for this can be found <a href="http://resources.mpi-inf.mpg.de/tmo/logmap/logmap.pdf" rel="noopener">here</a>.</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*0gbgFTFg_mlZRqdWa7b8qw.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><p>You asked for it</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*9Ss_5MkHidaiJcIU67vHUg.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Results</figcaption></figure><h3 id="ongoing-research-in-generating-hdr-content">Ongoing research in generating HDR content</h3><p>The conventional approach of generating HDR content is by merging multiple images captured at different exposures (bracketing). However, this approach is likely to create ghosting (blur) artifacts when there is movement between the frames. This has been solved by first aligning the neighbouring frame with the reference frame (middle frame) using something known as <em>optical flow</em>. That can be a topic for another blog post but for now we can think of it as a way to estimate the motion of objects (or pixels) that happens across frames by assigning a displacement vector to certain pixel positions.</p><p>There also has been work in generating HDR frames from singular LDR counterparts using Deep Learning. Neural networks are successfuly able to learn complex representations between the input and the output, and have thus performed quite well in learning the LDR to HDR mapping. These are some of the state-of-the-art methods for HDR image generation from a single image ‚Äî</p><ul><li>HDRCNN: <a href="http://hdrv.org/hdrcnn/material/sga17_paper.pdf" rel="noopener">HDR image reconstruction from a single exposure using deep CNNs</a></li><li><a href="https://arxiv.org/pdf/1903.01277.pdf" rel="noopener">Deep Inverse Tone Mapping Using LDR Based Learning for Estimating HDR Images with Absolute Luminance</a></li><li><a href="https://arxiv.org/abs/1803.02266" rel="noopener">ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content</a></li></ul><p>Here are some state-of-the-art Deep Learning based methods for HDR image generation using multiple LDR images ‚Äî</p><ul><li><a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Deep High Dynamic Range Imaging of Dynamic Scenes</a></li><li>AHDRNet: <a href="https://arxiv.org/abs/1904.10293" rel="noopener">Attention-guided Network for Ghost-free High Dynamic Range Imaging</a></li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://towardsdatascience.com/paper-review-attention-guided-network-for-ghost-free-high-dynamic-range-imaging-4df2ec378e8"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Paper review‚Ää‚Äî‚ÄäAttention-guided Network for Ghost-free High Dynamic Range Imaging</div><div class="kg-bookmark-description">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at CVPR 2019‚Ä¶</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png" alt="HDR Imaging: What is an HDR image anyway? üì∑"><span class="kg-bookmark-author">Mukul Khanna</span><span class="kg-bookmark-publisher">Towards Data Science</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://miro.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" alt="HDR Imaging: What is an HDR image anyway? üì∑"></div></a></figure><h3 id="how-to-view-hdr-images">How to view HDR images</h3><p>HDR images are stored as luminance maps and not as conventional RGB images and thus can‚Äôt be viewed using common image viewing applications.</p><p>MacOS allows you to view .hdr and .exr files using the Preview and Finder app. You can also use the <a href="https://viewer.openhdr.org/" rel="noopener">OpenHDR</a> website to visualise such images.</p><hr><p>Thanks for reading.</p><p>For future blog posts, I would like to discuss about how operations can be performed on HDR images using Python, OpenCV and Numpy. I would also like to share the current scenario of research being done for HDR video generation.</p>]]></content:encoded></item><item><title><![CDATA[Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë]]></title><description><![CDATA[<figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" class="kg-image" alt><figcaption>Source: <a href="https://donggong1.github.io/ahdr" data-href="https://donggong1.github.io/ahdr" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet)</a></figcaption></figure><blockquote><em>‚ÄúYou just want attention, you don‚Äôt want my heart‚Äù‚Ää‚Äî‚ÄäCharlie Puth</em></blockquote><h3 id="about-the-paper">About the paper</h3><p><em>Attention-guided Network for Ghost-free High Dynamic Range Imaging</em> (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at</p>]]></description><link>http://localhost:2368/attention-guided-network-for-ghost-free-high-dynamic-range-imaging-cvpr-2019-paper-review/</link><guid isPermaLink="false">5f93a650f123610eb9e42a29</guid><category><![CDATA[1k+ views]]></category><category><![CDATA[Paper Review]]></category><category><![CDATA[HDR]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Tue, 24 Sep 2019 04:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1549545931-59bf067af9ab?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Source: <a href="https://donggong1.github.io/ahdr" data-href="https://donggong1.github.io/ahdr" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet)</a></figcaption></figure><blockquote><em>‚ÄúYou just want attention, you don‚Äôt want my heart‚Äù‚Ää‚Äî‚ÄäCharlie Puth</em></blockquote><h3 id="about-the-paper">About the paper</h3><img src="https://images.unsplash.com/photo-1549545931-59bf067af9ab?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><p><em>Attention-guided Network for Ghost-free High Dynamic Range Imaging</em> (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at CVPR 2019, and can be read <a href="https://arxiv.org/abs/1904.10293" rel="noopener">here</a>. The primary author, <a href="https://donggong1.github.io/" rel="noopener">Dong Gong</a> is a Postdoctoral researcher at The University of Adelaide. His interests include machine learning and optimization in Computer Vision.</p><blockquote><strong><em>Note</em></strong><em>: Image results, network representations, formulas and tables used in this blog post have all been sourced from the </em><a href="https://arxiv.org/abs/1904.10293" rel="noopener"><em>paper</em></a><em>.</em></blockquote><h3 id="el-problema">El problema</h3><p>For generation of an HDR image from multi-exposure bracketed LDR images, alignment of the LDR images is very important for dynamic scenes with in-frame motion. Misalignments not accounted for before merging causing ghosting (among other) artifacts. There have been several successful (almost) attempts at compensating this motion between frames by using Optical flow estimation. As it turns out, the shortcomings of flow based methods have not served the HDR cause well.</p><p>This can be seen in an attempt by <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>, where regardless of the accurate spatial reconstruction in saturated image regions, alignment artifacts can be observed for input frames with severe motion. This can be seen in the results provided by the authors of AHDRNet in the image below. Another attempt at HDR reconstruction that targets specifically at highly dynamic LDR bracketed input (<a href="https://arxiv.org/pdf/1711.08937.pdf" rel="noopener">Wu et. al</a>) claims to exploit the apparent prowess of CNN architectures in learning misalignments and compensating for the ghosting artifacts. Results presented below however show that there is scope for improvement.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*30oeBm3mjtt2UsDCSqu0gA.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><h3 id="attention-to-the-rescue">Attention to the rescue</h3><p>The authors propose to use attention mechanisms to solve this bi-faceted problem of reduction of alignment artifacts + accurate HDR reconstruction by using attention mechanisms. If you think about it, attention networks are just a very few Conv layers stacked together, followed (usually) by a sigmoid activation that allows the networks to focus on what‚Äôs important and pertinent to the application.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*kOhxdmePmBQfgrhhKMiwwA.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>Here, the attention networks are utilised to suppress the alignment artifacts and to focus on infusing out better-exposed image regions into the generated image by attending to the spatial dynamics of the bracketed images with respect to the reference image. Regions which correspond to the reference image are highlighted, whereas regions with severe motion and saturation are suppressed. We‚Äôll see how the attention information matrix is processed and implemented, in terms of the mathematics behind it, in some time.</p><p>The attention part of the network focuses on making decisions about which image regions contribute better to the accuracy of the network output. This is followed by a merge network that based on the attention output, tries to create HDR content from the LDR input. The better the attention mechanism, the better is the input to the merge network, thus allowing it to utilise information in the more relevant parts of the input. The merge network has been developed using dilated dense residual blocks that improve gradient-flow, hierarchical learning and convergence. The whole network is trained in an end-to-end fashion and therefore both subnetworks mutually influence each other, and learn together.</p><h3 id="implementation">Implementation</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*IJfbHMvhiFWC5yQe6PCxxg.jpeg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Overview</figcaption></figure><h4 id="preprocessing">Preprocessing</h4><p>The non-linear LDR input (<em>I1, I2 , I3</em>) is transferred to the linear domain by applying an inverse CRF (gamma correction here) and dividing by their corresponding exposure times.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*iy2Do0TiEa70L4ZZOrUU1A.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Œ≥ =&nbsp;2.2</figcaption></figure><p>Both the linear and non-linear input (<em>Ii, Hi</em>) are concatenated along the channel dimensions to form <em>Xi</em>. <em>X1</em> , <em>X2</em> and <em>X3</em> are fed to the network to generate the corresponding HDR output.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*MgAQuh0sfypm-2aEUM0g6A.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption><em class="markup--em markup--figure-em">H</em> is the generated image, <em class="markup--em markup--figure-em">f</em> (.) represents the AHDRNet network and <em class="markup--em markup--figure-em">Œ∏</em> the network parameters</figcaption></figure><p>The network performs better when it has the linearised input information at its disposal. This has been observed and utilised in <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a> as well as <a href="https://arxiv.org/pdf/1711.08937.pdf" rel="noopener">Wu et. al</a>.</p><h3 id="architecture">Architecture</h3><p>The whole network comprises of two sub-networks‚Ää‚Äî‚Ääattention network and merging network.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*FHnHzLbQz58BHcRMFhOd1w.jpeg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Attention network</figcaption></figure><p>The attention network, as discussed above helps in avoiding alignment artifacts by highlighting and using information from regions in the neighbouring images (non-reference images) that correspond to the reference image. It does so in the following way.</p><p>Attention is not extracted from and applied directly to the concatenated image pairs. First the <em>Xi</em> s are passed through a Conv layer to extract a 64 channel feature map <em>Zi</em>.</p><p>Then, the reference feature map (<em>Z2</em> or <em>Zr</em>) along with a neighbouring image feature map (<em>Z1</em> and <em>Z3</em>) is fed into the attention module that generates an attention map to mark the important regions in the non-reference feature map with reference to <em>Zr</em>.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*TeylpymAABwM2B6X.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>This is done for both the pairs‚Ää‚Äî‚Ää(<em>Z1</em> , <em>Z2</em>) and (<em>Z3</em> , <em>Z2</em>). This can be seen clearly in the above network representation.</p><p>Before we get into what the attention module does, let‚Äôs see what to do with the attention map that is generated. The attention map generated is essentially a 64-channel matrix that contains values between [0,1]. This matrix serves as a kind of a weight-matrix, in which each element represents the importance of a corresponding element in the feature matrix of the neighbour image, with reference to <em>Z2</em>. This is implemented by using the attention map generated from (<em>Z1</em> , <em>Z2</em>) by doing an element wise multiplication of the attention map and <em>Z1</em> to get <em>Z‚Äô1</em>.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*AyY92BJFOzyEAatL.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>This operation results in important features (where attention is closer to 1) in <em>Z1</em> getting higher numerical values and correspondingly lower values for less important features. This manifests in only important image regions from <em>Z1</em> going ahead in the network to contribute to the final HDR output. The same thing happens between (<em>Z3</em> , <em>Z2</em>) to get <em>Z‚Äô3</em>.</p><p>Now that we have all the input pieces most relevant to construct the HDR image, we concatenate these along the channel dimension as below -</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*oulHopStLvaePUth.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><h4 id="attention-module">Attention module</h4><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*VJ5ooOqafNOC5v7O.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>Let‚Äôs see how these attention maps are generated. The attention module used in this paper comprises of 2 Conv2d layers that output a 64-channel matrix, followed by a ReLU and a sigmoid activation respectively. It takes as input a concatenated feature vector of neighbouring and reference image (2 x 3 = 6 channels). The sigmoid activation, in the end, is used to contain the output in a [0,1] range.</p><h4 id="attention-results">Attention results</h4><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*op6pjaA5-eo-eecZ.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Attention map examples from the paper. ( a ) to ( c )‚Äî Alignment attention; ( d ) to ( f )‚Ää‚Äî‚ÄäExposure attention</figcaption></figure><p>In ( a ) to ( c ), it can be observed from the results above how regions with motion discrepancies in non-reference images are suppressed (darker blue region) whereas regions that have correspondances with the reference image are highlighted (brighter blue-ish green). In ( d ) to ( f ), the regions that are better exposed in the neighbouring frames are highlighted and saturated regions are suppressed.</p><h3 id="merging-network">Merging network</h3><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*nq0yC0HIakfLZcdZ.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>The concatenated feature map (<em>Zs</em>) is given as input to the merging network. The merging network used by the authors is the residual dense network proposed in <a href="https://arxiv.org/abs/1802.08797" rel="noopener">Zhang et. al</a>. Instead of the conventional Conv operations, the authors have used dilated convolutions to propagate a larger receptive field, thus calling it a Dilated Residual Dense Block (DRDB). There are 3 such blocks in the merging network that consist of dense concatenation based skip-connections and residual connections that have been proved quite effective for CNNs in solving gradient vanishing, allowing better backpropagation, hierarchical learning and therefore aiding and improving convergence performance. In the proposed AHDRNet network, each DRDB consists of 6 Conv layers and a growth rate of 32.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*1ZrNm30PPxTHo9Iw.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>DRDB</figcaption></figure><p>The authors have also employed local and global residual skip connections that bypass low level features to higher level ones. Local residual learning is implemented within the DRDBs whereas global residual learning is for transferring the shallow feature maps containing <em>pure</em> information from the reference image to the latter stages. This, and other network specifications can be observed in the merging network diagram.</p><h3 id="loss-functions">Loss functions</h3><p>Just like <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>, the loss is calculated between <em>Œº</em>-law tonemapped generated and tonemapped ground truth images. <em>Œº</em> has been set to 5000 for all the experiments. The <em>Œº</em>-law can be defined as -</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*W_ebdoq-1Dus37Gi.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption><em class="markup--em markup--figure-em">Œº</em>-law</figcaption></figure><p>An L1 loss has been used for the same. Quantitative comparisons PSNR and HDR-VDP-2 scores presented in the paper convey that L1 loss is better at reconstructing finer details as compared to an L2 loss.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*In81bfClnA_qHrlV.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><h4 id="implementation-specifications">Implementation specifications</h4><p>The architecture was implemented using PyTorch. The specifications and hyper-parameters are -</p><ul><li>Weight initialisation: Xavier</li><li>Optimizer: ADAM</li><li>Learning rate: 1 x 10‚Äì5</li><li>Batch size: 8</li><li>GPU: NVIDIA GeForce 1080 Ti</li><li>Inference time for 1 image (1500x1000): 0.32 sec</li></ul><h4 id="results">Results</h4><p>The networks were trained and tested on the datasets provided by <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>. The authors have provided quantitative and qualitative comparisons between several variants of the network, in terms of PSNR and HDR-VDP-2 scores.</p><ul><li>AHDRNet‚Ää‚Äî‚ÄäThe full model of the AHDRNet.</li><li>DRDB-Net (i.e. AHDRNet w/o attention)</li><li>A-RDB-Net (i.e. AHDRNet w/o dilation)</li><li>RDB-Net (i.e. AHDRNet w/o attention and dilation)</li><li>RB-Net (i.e. AHDRNet w/o attention, dilation and dense connections). DRDBs replaced by RBs.</li><li>Deep-RB-Net. More RBs are used.</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*EUnWQxWutnMEJg6i.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>The results show how each component of the AHDRNet is important to the efficacy of the whole network i.e. attention is important, dilated convolutions are important, dense connections are important and residual learning is important.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*WC-uKJVWhFLumxbb.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Visual results, from the&nbsp;paper</figcaption></figure><p>Comparison with state-of-the-art</p><p>Comparisons with current state-of-the-art approaches (learning based and non-learning based) reveal how AHDRNet beats existing approaches. The closest competitor is obviously <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>‚Äôs implementation that comes only second to AHDRNet. The authors have also provided the results of a variant of AHDRNet that uses Optical-flow aligned images (AHDRNet + OF).</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*iWVJeN5urpZVz9Ka.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>The visual results show the efficacy of the network in infusing meticulous details in the generated HDR output without giving rise to any alignment artifacts even in cases of severe motion. Here are some of the results, taken from the paper -</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*muRv_BXdBEHgJOxd.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*H1TAnwS6l0whMraX.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><h3 id="conclusion">Conclusion</h3><p>AHDRNet is the first attention-based approach to solving the problem of HDR image generation. The finesse of attention mechanisms has been utilised to align the input LDR images. Previous attempts at image alignment have used Optical flow-based methods, which have some inaccuracies and do not perform well for severe motion between frames. An attention-based approach however performs exceedingly well in terms of HDR reconstruction as well as in removing alignment artifacts. Extensive experiment reveal how AHDRNet supersedes existing approaches qualitatively and quantitatively, and that it has become the new state-of-the-art in HDR image generation.</p><h3 id="references">References</h3><ul><li>Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, Yanning Zhang, <a href="https://arxiv.org/abs/1904.10293" rel="noopener">Attention-guided Network for Ghost-free High Dynamic Range Imaging</a> (2019), CVPR 2019</li><li>N. K. Kalantari and R. Ramamoorthi, <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Deep high dynamic rangeimaging of dynamic scenes</a> (2017), ACM Trans. Graph</li><li>Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang, <a href="https://arxiv.org/abs/1711.08937" rel="noopener">Deep high dynamic range imaging with large foreground motions</a> (2018), European Conference on Computer Vision (ECCV), September 2018</li></ul>]]></content:encoded></item><item><title><![CDATA[How you can test your Vue.js apps in less than 7 minutes üî¨]]></title><description><![CDATA[<figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*PkFvOQVwXsa-Rsd004WyDQ.png" class="kg-image" alt></figure><p>Before we dive into the implementation, let‚Äôs get a few concepts cleared.</p><h3 id="what-is-testing">What is testing?</h3><p>Manually trying all possible inputs to a basic form validator can be cumbersome.</p><p>It might not seem like a big deal for a small website. But for bigger and more complex web applications consisting</p>]]></description><link>http://localhost:2368/vuejs-testing-in-less-than-7-minutes/</link><guid isPermaLink="false">5f93af5ef123610eb9e42a94</guid><category><![CDATA[16k+ views]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Tue, 13 Feb 2018 04:47:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1557948206-7478d769f813?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*PkFvOQVwXsa-Rsd004WyDQ.png" class="kg-image" alt="How you can test your Vue.js apps in less than 7 minutes üî¨"></figure><img src="https://images.unsplash.com/photo-1557948206-7478d769f813?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="How you can test your Vue.js apps in less than 7 minutes üî¨"><p>Before we dive into the implementation, let‚Äôs get a few concepts cleared.</p><h3 id="what-is-testing">What is testing?</h3><p>Manually trying all possible inputs to a basic form validator can be cumbersome.</p><p>It might not seem like a big deal for a small website. But for bigger and more complex web applications consisting of dozens of components along with their functions, routes, states, mutations and so on, it is not feasible or advisable to test the functioning of all these constituents.</p><p>Automating this part of the trial and error based assessments of the code we have written is known as <strong>testing</strong> or <strong>automated testing</strong>.</p><p>Edd Yerburgh, a core Vue team member and the maintainer of vue-test-utils (formerly <strong>Avoriaz</strong>), defines automated testing in his <a href="https://livebook.manning.com#!/book/testing-vuejs-applications/chapter-1/v-3/point-1371-28-28-0" rel="noopener">book</a> as:</p><blockquote>Automated testing is the practice of writing programs to run tests against your application code. Once the programs are written, they can be executed automatically.</blockquote><p>There are essentially three types of tests:</p><ol><li>Unit tests</li><li>End to end tests</li><li>Snapshot tests</li></ol><h4 id="unit-tests"><strong>Unit tests</strong></h4><p>These are basic tests that check if the atomic elements of the website (Vue components and functions) work properly. Edd calls them <strong>component contracts</strong>. Each component is expected to work as it has promised to do, and these tests make sure that they are fulfilled.</p><h4 id="end-to-end-e2e-tests"><strong>End to end (E2E) tests</strong></h4><p>E2E tests test the whole workflow of the website. It can be said that one E2E test is made up of multiple granular unit tests. They are slow, but they check the whole functionality of the website.</p><p>But they are also difficult to debug because it‚Äôs tough to locate which parts didn‚Äôt work as they were supposed to. There could be more than one reason that the tests failed.</p><h4 id="snapshot-tests">Snapshot tests</h4><p>Bugs in the code don‚Äôt only affect the functionality of the website, but also the positioning of the components in the UI. Snapshot tests check for such changes in the appearance of the application. It involves rendering the UI, capturing a screenshot, and comparing it to a reference image stored along with the test. The test fails if the two images don‚Äôt match.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*4Tw2yL3mzDvM8FA8Mcwmrg.jpeg" class="kg-image" alt="How you can test your Vue.js apps in less than 7 minutes üî¨"><figcaption><a href="https://livebook.manning.com/#!/book/testing-vuejs-applications/chapter-1/v-3/156" data-href="https://livebook.manning.com/#!/book/testing-vuejs-applications/chapter-1/v-3/156" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">The testing&nbsp;pyramid</a></figcaption></figure><p>These tests also help developers write proper documentation of the code, which is quite useful in large scale applications with multiple contributors.</p><p>So now that we‚Äôve established that testing can help us save a lot of time and optimize our code, let‚Äôs see how tests are configured, created, and run.</p><p>We will be using <strong>vue-test-utils</strong> as the testing utility library for Vue.js<em>. </em>Now we also need to choose a test runner. There are many to choose from, but Jest and Mocha-Webpack are both equally good. They just have some tradeoffs between the configuration upfront and the support for SFCs (single file components).</p><p>We will be using the <strong>mocha-webpack</strong> configuration for this demo.</p><h3 id="creating-the-projectnpm-install-vuenpm-install-global-vue-clivue-init-webpack-vue-testingcd-vue-testingnpm-installnpm-run-dev"><strong>Creating the project</strong>npm install vuenpm install --global vue-clivue init webpack vue-testingcd vue-testingnpm installnpm run dev</h3><p>Using the above commands, create a Vue webpack project in which we will be setting up the testing environment.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*hW9NereuY6BpbKAwKIiKRg.jpeg" class="kg-image" alt="How you can test your Vue.js apps in less than 7 minutes üî¨"></figure><h4 id="installing-dependencies"><strong>Installing dependencies</strong></h4><p>To install <a href="https://github.com/vuejs/vue-test-utils" rel="noopener">vue-test-utils</a><em>, </em>mocha,<em> </em>and mocha-webpack:</p><pre><code>npm install --save-dev @vue/test-utilsnpm install --save-dev mocha mocha-webpack</code></pre><p>To emulate a subset of a browser environment to run our tests, we‚Äôll install <a href="https://github.com/jsdom/jsdom" rel="noopener">jsdom</a><em> </em>and <a href="https://github.com/rstacruz/jsdom-global" rel="noopener">jsdom-globa</a>l:</p><pre><code>npm install --save-dev jsdom jsdom-global</code></pre><p>Some of the dependencies that we will be importing in our tests are difficult for the webpack to bundle. So, to be able to remove them from the bundling process and to increase test bootup speed, we install <strong>node-externals:</strong>npm install --save-dev webpack-node-externals</p><p>Vue recommends <a href="https://github.com/Automattic/expect.js" rel="noopener">expect</a> as an assertion library that essentially decides whether the test fails or passes depending on the argument it receives.</p><pre><code>npm install --save-dev expect</code></pre><p>We need to make it globally accessible to avoid importing it in every single test. We create a directory named <strong>test</strong><em> </em>in the root directory and create a file named <strong>test/setup.js</strong><em> . </em>Import the modules with <strong>require</strong><em>:</em></p><pre><code class="language-Javascript">//setup.js

require('jsdom-global')()
global.expect = require('expect')</code></pre><p>We can also include code coverage in the test results using the <strong>istanbul</strong><em> </em>plugin<em> </em>to get a report like this:</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*1l1xw55caGD-W-akmUCnbg.jpeg" class="kg-image" alt="How you can test your Vue.js apps in less than 7 minutes üî¨"></figure><p>It is used to describe the degree to which the source code of an application is executed when a particular test suite runs.</p><pre><code>npm install --save-dev nyc babel-plugin-istanbul</code></pre><p>Also in the <strong>.babelrc</strong><em> </em>in the <strong>plugins</strong><em> </em>array, add <strong>istanbul:</strong></p><pre><code>//.babelrc

plugins": ["transform-vue-jsx", "transform-runtime", "istanbul"]</code></pre><p>So we have installed all the dependencies, and it‚Äôs time to make the final configurations before we can start writing the tests.</p><p>In <strong>package.json</strong>, we need to add a <strong>test</strong><em> </em>script that runs the test:</p><pre><code class="language-JSON">//package.json

"scripts":{
	"test": "cross-env NODE_ENV=test nyc mocha-webpack --webpack-config build/webpack.base.conf.js --require test/setup.js test/**/*.spec.js"
}</code></pre><p>We also need to specify the files that needed to be included for the code coverage in the <strong>package.json:</strong></p><pre><code class="language-JSON">//package.json

"nyc":{
    "include":[
      "src/**/*.(js|vue)" ],
    "instrument":false,
    "sourceMap":false
}</code></pre><p>The last configuration before writing the test would be adding the following in <strong>webpack.base.conf.js:</strong></p><pre><code class="language-Javascript">//webpack.base.conf.js

if (process.env.NODE_ENV === 'test'){

  module.exports.externals = [require('webpack-node-externals')()]
  module.exports.devtool = 'inline-cheap-module-source-map'

}</code></pre><p>We can perform our test on the inbuilt Vue component that comes with the webpack boilerplate.</p><p>Every test file would have a <strong>‚Äò.spec.js‚Äô</strong><em> </em>extension.</p><p>In the test directory, we add a test file <strong>testOne.spec.js -</strong></p><pre><code class="language-Javascript">//testOne.spec.js

import {shallow} from '@vue/test-utils'
import HelloWorld from '../src/components/HelloWorld.vue'
</code></pre><p>We start by importing <strong>shallow</strong><em> </em>from the <strong>vue-test-utils</strong><em>. </em><strong>Shallow</strong><em> </em>creates a <a href="https://vue-test-utils.vuejs.org/en/api/wrapper/" rel="noopener">wrapper</a> for the Vue component on which we want to run the test. This wrapper is an object that contains the mounted component and methods to test parts of the 	code. Then we import the Vue component on which we run the test.</p><pre><code class="language-Javascript">//testOne.spec.js

describe('HelloWorld.vue',function(){        
	it('Checking &lt;h2&gt; tag text',function(){                
    	const wrapper = shallow(HelloWorld)        
        const h2= wrapper.find('h2')        
        expect(h2.text()).toBe('Essential Links')        
    })
})</code></pre><p>Then we create what we can call a <strong>test suite</strong>, using the <strong>describe()</strong><em> </em>method of Mocha‚Äôs testing framework. This test suite basically groups multiple test cases into one along with providing some information about the tests and the component.</p><p>In this describe function, we callback a function that specifies the test cases using the <strong>it() </strong>function. Each it() method describes a test case with the purpose of the test as the first parameter followed by a callback function defining the test.</p><p>Then:</p><ul><li>We create a wrapper of the Vue component</li><li>Use its <strong>find()</strong><em> </em>method to get all &lt;h2&gt; tag elements</li><li>Compare its text with what it is supposed to be.</li></ul><p>Yay! Our test is ready to run.npm run test</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*aRLjKH_Rts_8T4NgdvmwiQ.jpeg" class="kg-image" alt="How you can test your Vue.js apps in less than 7 minutes üî¨"></figure><p>So, our test was successful‚Ää‚Äî‚Ääthe code was able to find an &lt;h2&gt; tag in the HelloWorld.vue component with ‚ÄòEssential Links‚Äô as its text.</p><p>Now if we change the expected test to anything else, the test would fail. <br>I changed it to:</p><pre><code>expect(h2.text()).toBe('Essential Linx')</code></pre><p>and the test fails. The failed test error is quite descriptive, though, and you can see what the code was expecting and what it receives:</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*-gttMaktDFEOKt29IDUaBw.jpeg" class="kg-image" alt="How you can test your Vue.js apps in less than 7 minutes üî¨"></figure><p>We can add multiple test cases in one test file by using multiple <strong>it()</strong> methods and expecting different conditions.</p><pre><code class="language-Javascript">describe('HelloWorld.vue',function(){  

it('Checking &lt;h2&gt; tag text',function(){
        const wrapper = shallow(HelloWorld)        
        const h2 = wrapper.find('h2')
        expect(h2.text()).toBe('Essential Links')    
    }),    
it('Checking &lt;h1&gt; tag text',function(){
        const wrapper = shallow(HelloWorld)
        const h1 = wrapper.find('h1')
        expect(h1.text()).toBe('Welcome to Your Vue.js App')    
    })
})</code></pre><p>Here we are also testing if the &lt;h1&gt; tag renders what it is supposed to.</p><p>So this was a pretty basic test that just gives you an understanding of how tests are configured, coded, and run without even opening the browser or starting the server.</p><p>The link to the GitHub repository is <a href="https://github.com/mukulkhanna/vue-testing" rel="noopener">here</a>.</p><h3 id="wrapping-up">Wrapping up</h3><p>Edd Yerburgh‚Äôs book ‚Äò<a href="https://www.manning.com/books/testing-vuejs-applications" rel="noopener">Testing Vue.js Applications</a>‚Äô helped me a lot in getting a wider picture of the importance of testing and how to implement it. I would recommend it to anyone who wants to learn testing beyond the scope of beginner-level content and really dive into it.</p><p>Other than that, I have been spending some time on TDD (Test Driven Development) concepts and am looking forward to writing a beginner‚Äôs tutorial about the world of TDD with Vue.js.</p>]]></content:encoded></item><item><title><![CDATA[Creating Underwater effects in Unity3D ü§ø]]></title><description><![CDATA[<p>So I started using Unity to create a simulation environment for a college team project. I had to create a swimming pool like structure where I could navigate a Capsule Solid RigidBody inside the water. It took me quite some time to implement, so hoping this helps someone out.</p><h3 id="constructing-the-pool">CONSTRUCTING</h3>]]></description><link>http://localhost:2368/creating-underwater-effects-in-unity/</link><guid isPermaLink="false">5f93b2ebf123610eb9e42af5</guid><category><![CDATA[11k+ views]]></category><category><![CDATA[Simulation]]></category><category><![CDATA[Underwater]]></category><category><![CDATA[Unity3D]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Mon, 25 Dec 2017 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1547955922-85912e223015?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1547955922-85912e223015?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Creating Underwater effects in Unity3D ü§ø"><p>So I started using Unity to create a simulation environment for a college team project. I had to create a swimming pool like structure where I could navigate a Capsule Solid RigidBody inside the water. It took me quite some time to implement, so hoping this helps someone out.</p><h3 id="constructing-the-pool">CONSTRUCTING THE POOL</h3><p>Setting up a pool by joining the cuboids appropriately didn‚Äôt take much time.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*5_X4vJgy8DJl0L0dunMxVQ.jpeg" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"><figcaption>Making a pool with cuboids-1</figcaption></figure><p>I have used a purple plane like cuboid as the base/floor for the pool for consistency reasons.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*ibXBkZOi1dob_kE4XygUow.jpeg" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"><figcaption>Making a pool with cuboids-2</figcaption></figure><h3 id="adding-water-prefab"><strong>ADDING WATER PREFAB</strong></h3><p>Next step would be to add a water surface. Water prefabs like WaterProDaytime, Water4Simple etc use an oval-shaped mesh for the water. If we want to use a different mesh, we can change it in the <em>Mesh Filter</em> of the water GameObject.</p><p>I have used <strong>WaterProDaytime</strong> for the environment.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*uIaYnKKwzMk2FrN7k6RVUA.jpeg" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"><figcaption>WaterProDaytime prefab for pool&nbsp;water</figcaption></figure><p>The look and feel of the water surface can be modified from the inspector panel where we can choose the Shaders, colors, wave speed etc. for the water.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*erz5rLQzKKCbx4qVuuvQCA.jpeg" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"></figure><p>Now that we have added, the water surface, lets add a Capsule GameObject that we will be controlling.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*72neRSVOqIirwC86T46WsA.gif" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"></figure><p>Now as we can see, when the capsule is underwater, it doesn‚Äôt seem like it. Because till now, the prefab acts like a surface of water and not a water body.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*kAH6mi6XDW-slrjowBJhYA.jpeg" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"></figure><p>Also, from below the water, the surface is not visible. Now to be able to do that, we need to add one more Prefab (copy-paste the original one), and rotate it 180<strong>¬∞ </strong>along the X axis.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*ZKXsWn3H5rJFt70iDP4sBg.jpeg" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"></figure><p>We need to realise that two primary things simulate an underwater effect-</p><ol><li>A foggy effect with fog color and a fog density values.</li><li>A caustics generated texture.</li></ol><h3 id="foggy-effect">FOGGY EFFECT</h3><p>To be able to get the visual effect, we need to add a C# script to the Camera GameObject that has been positioned underwater. The script is pretty self explanatory-</p><pre><code class="language-C#">using UnityEngine;
using System.Collections;
 
public class Underwater: MonoBehaviour {
	public float waterHeight;
 
	private bool isUnderwater;
	private Color normalColor;
	private Color underwaterColor;
 
 	// Use this for initialization
    void Start () {
		normalColor = new Color (0.5f, 0.5f, 0.5f, 0.5f);
		underwaterColor = new Color (0.22f, 0.65f, 0.77f, 0.5f);
 	}
 
	// Update is called once per frame
	void Update () {
		if ((transform.position.y &lt; waterHeight) != isUnderwater) {
			isUnderwater = transform.position.y &lt; waterHeight;
 			if (isUnderwater) SetUnderwater ();
 			if (!isUnderwater) SetNormal ();
 		}
 	}
 
 	void SetNormal () {
		RenderSettings.fogColor = normalColor;
		RenderSettings.fogDensity = 0.01f;
 
	}
 
 	void SetUnderwater () {
		RenderSettings.fogColor = underwaterColor;
 		RenderSettings.fogDensity = 0.1f;
 	}
 }</code></pre><h3 id="generating-caustics">GENERATING CAUSTICS</h3><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*3M3_I6q6M8W4LgiEV5HkAA.png" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"></figure><p>The effect generated by caustics can be seen as an array of similar images, displayed one after another, one per frame. We can generate these images from applications like <a href="https://www.dualheights.se/caustics/" rel="noopener">this</a>.</p><p>Now we need a GameObject that can project these images on the water. We can use a <em>LightProjector</em> to do so and attach a script that takes input of images in an array and renders them.</p><pre><code class="language-C#">using System.Collections;
using System.Collections.Generic;
using UnityEngine;
 
public class WaterEffect: MonoBehaviour {
 
	public float fps=30.0f;         //footage fps
	public Texture2D[] frames;      //caustics images
 
 	private int frameIndex;
 	private Projector projector;    //Projector GameObject
 
 	void Start(){
 		projector = GetComponent&lt;Projector&gt; ();
        	NextFrame();
        	InvokeRepeating ("NextFrame", 1 / fps, 1 / fps);
 	}
 
 	void NextFrame(){
     		projector.material.SetTexture ("_ShadowTex", frames [frameIndex]);
     		frameIndex = (frameIndex + 1) % frames.Length;
 	}	
 
 }</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*hjaV8uAlzcYMVCTB2Ukonw.png" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"><figcaption>Loading images into the&nbsp;array</figcaption></figure><p>Now all we need to do is to place the projector appropriately so that the caustics can be projected across the whole water body.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*YRvRa_haThe2LFjRuLNjLA.gif" class="kg-image" alt="Creating Underwater effects in Unity3D ü§ø"><figcaption>Final underwater appearance</figcaption></figure><p>I had very a limited time frame to complete this task and certain links and videos on the internet helped a lot. They are-</p><ul><li><a href="https://www.youtube.com/watch?v=FoZwgRE5LYI" rel="nofollow noopener noopener">https://www.youtube.com/watch?v=FoZwgRE5LYI</a></li><li><a href="https://youtu.be/GHYUJO8P4_Y" rel="nofollow noopener noopener">https://youtu.be/GHYUJO8P4_Y</a></li></ul>]]></content:encoded></item></channel></rss>