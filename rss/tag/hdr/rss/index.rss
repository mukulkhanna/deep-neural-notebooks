<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="//purl.org/dc/elements/1.1/" xmlns:content="//purl.org/rss/1.0/modules/content/" xmlns:atom="//www.w3.org/2005/Atom" version="2.0" xmlns:media="//search.yahoo.com/mrss/"><channel><title><![CDATA[HDR - Deep Neural Notebooks]]></title><description><![CDATA[Research, Insights & Stories]]></description><link>/</link><image><url>/favicon.png</url><title>HDR - Deep Neural Notebooks</title><link>/</link></image><generator>Ghost 3.21</generator><lastBuildDate>Wed, 11 Nov 2020 11:07:16 GMT</lastBuildDate><atom:link href="/tag/hdr/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[HDR Imaging: What is an HDR image anyway? üì∑]]></title><description><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/0*E1pzpUrIPoMfaWNg.jpg" class="kg-image" alt></figure><p>We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn‚Äôt turn out well. The image comes out to be either too dark or too bright depending on the focus. Let‚Äôs try to understand why this happens and how this</p>]]></description><link>/what-is-an-hdr-image-anyway/</link><guid isPermaLink="false">5f93a47ef123610eb9e42a04</guid><category><![CDATA[4k+ views]]></category><category><![CDATA[HDR]]></category><category><![CDATA[Computer Vision]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Thu, 10 Oct 2019 03:56:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/0*E1pzpUrIPoMfaWNg.jpg" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><img src="https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="HDR Imaging: What is an HDR image anyway? üì∑"><p>We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn‚Äôt turn out well. The image comes out to be either too dark or too bright depending on the focus. Let‚Äôs try to understand why this happens and how this can be solved.</p><p>There are a lot of key concepts that revolve around the study of HDR images -</p><ul><li><strong>Dynamic range</strong></li><li><strong>Image exposure</strong></li><li><strong>Shutter speed, Aperture, ISO</strong></li><li><strong>Image bracketing</strong></li><li><strong>Merging LDR images</strong></li><li><strong>Image encoding</strong></li><li><strong>Camera response function</strong></li><li><strong>Linearisation</strong></li><li><strong>Gamma correction</strong></li><li><strong>Tonemapping</strong></li><li><strong>Visualising HDR images</strong></li></ul><h3 id="dynamic-range">Dynamic Range</h3><p>Dynamic range of a scene refers to the range of light intensity that encompasses a scene. It can also be defined as the ratio of light (maximum measurable brightness) to dark (minimum measurable brightness) in an image.</p><p>To get some context of the how brightness can be quantised, the range of light intensity is 0 to infinity, with zero being the darkest and infinity being the brightest source there is (‚òÄÔ∏è) .</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*YX1gBta3mldqWhvBMUmqsA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Luminance value comparison (<a href="https://www.hdrsoft.com/resources/dri.html" rel="noopener">Source</a>)</figcaption></figure><p>No camera is able to capture this complete uncapped range of illuminance in a scene. Therefore, images turn out to be either too bright (overexposed) or too dark (underexposed). These images are called <em>Low Dynamic Range</em> (LDR) images . For images that turn out too bright, it is only the brighter subrange (of the infinite range) that the camera is able to capture, and correspondingly for the darker images, only the lower subrange is captured.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*CzMITGf7VZ6ly2wLKz_d1g.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption><strong>True image</strong> (Left), <strong>Overexposed image</strong> (Center), <strong>Underexposed image</strong> (Right)</figcaption></figure><h3 id="image-exposure">Image Exposure</h3><p>The amount of light entering the camera (and thus, the image) is called the exposure. The exposure of the image can be controlled by three settings of a camera‚Ää‚Äî‚Ääthe aperture, shutter speed and ISO.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*mQ_fmmAzRkmKX63SrfCV8A.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><p><strong>Aperture:</strong> The area of the camera lens through which the light can come in.</p><p><strong>Shutter speed:</strong> The speed with which the shutter of the camera closes. As the shutter speed increases, the amount of light entering the camera decreases, and vice versa. It also improves the sharpness of an image.</p><p><strong>ISO: </strong>Sensitivity of the camera sensor to incoming light.</p><p>I found this nice analogy <a href="https://www.cambridgeincolour.com/tutorials/camera-exposure.htm" rel="noopener">here</a>, between the camera settings and a bucket left out in the rain.</p><blockquote>In photography, the exposure settings of aperture, shutter speed and ISO speed are analogous to the width, time and quantity in the discussion above. Furthermore, just as the rate of rainfall was beyond your control above, so too is natural light for a photographer.</blockquote><p>Back to <em>dynamic range</em>. A single image captured from the camera can not contain the wide range of light intensity.</p><p>This problem can be solved by merging images captured at multiple exposure values. How this helps is that the overexposed images work well for the darker regions in the image, and the underexposed images are able to tone down the intensity in the extra-bright regions. Different regions of the image are captured better at different exposure values. Therefore, the idea is to merge these set of images and to recover an image with a <em>high dynamic range</em> (HDR).</p><h3 id="image-bracketing">Image bracketing</h3><p>Bracketing refers to capturing multiple images of the same scene with different camera settings. It is usually done automatically by the camera. What happens when you use the HDR feature on your smart phone is that the phone captures 3 (usually) images at three different exposure times (or exposure values) in quick succession. The lower the exposure time, the lesser the amount of light that gets in. These three images are merged by the camera software and are saved as a single image, in a way that the best portions of each image make it to the final image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*gkqV3yHAhfQXEYGY.jpg" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>A collage of 5 bracketed images that I found on the <a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiR0PPA4JHjAhVQb30KHdH6DaUQjhx6BAgBEAM&amp;url=https%3A%2F%2Fwww.highdynamicranger.com%2Fhow-to-use-auto-exposure-bracketing-for-hdr%2F&amp;psig=AOvVaw0b8jjGtlbQ010BQW1YHPIQ&amp;ust=1562002928887447" rel="noopener">internet</a>.</figcaption></figure><p>The funny thing here is that the image that is saved on your phone after the merging happens, is still not (technically) an HDR image. This is where the image encodings come into the picture (and also tonemapping‚Ää‚Äî‚Ääwhich we‚Äôll discuss later).</p><h3 id="image-encoding">Image encoding</h3><p>Commonly, the images that we see on our phones and computers, are 8-bit (per channel) encoded RGB images. Each pixel‚Äôs value is stored using 24-bit representations, 8-bit for each channel (R, G, B). Each channel of a pixel has a range of 0‚Äì255 intensity values.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*KyT2vpk6-OCt_Nu9SXCNyw.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Example of 24-bit (3x8-bit) encoding for an RGB pixel</figcaption></figure><p>The problem with this encoding that it is not capable of containing the large dynamic range of natural scenes. It only allows a range of 0‚Äì255 (only integers) for accommodating the intensity range, which is not sufficient.</p><p>To solve this problem, HDR images are encoded using 32-bit floating point numbers, for each channel. This allows us to capture the wide uncapped range of HDR images. There are various formats for writing HDR images, the most common being .hdr and .exr. All HDR images are 32-bit encodings but not all 32-bit images can be HDR images.</p><h3 id="camera-response-function-crf-">Camera Response Function (CRF)</h3><p>CRF is a function that shows the relationship between the actual scene irradiance and the digital brightness values in the image. It is also called as the Opto-electrical transfer function. Camera companies don‚Äôt provide their CRFs and consider it as proprietary information.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*XA43PkIl_VE44Asilb5z2w.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Camera response function example</figcaption></figure><p>In an ideal world, the CRF should have a linear graph‚Ää‚Äî‚Äämeaning, the brightness value of the pixels in the image should be directly proportional to the actual irradiance in the scene. This is true for HDR images, but not for the usual images where the brightness values are altered to be able to contain them in a finite range. The more important reason for conventional images being de-linearised depends on how display devices work.</p><p>Back in the time of CRT (Cathode Ray Tube) displays, electrons were fired on a phosphor surface. The phosphor screen is known to emit photons upon being hit by accelerated electrons. However the brightness of the display didn‚Äôt vary linearly with the strength of the electron beam. This problem has been solved by modifying the incoming image/video source signals non-linearly in the direction opposite to the display‚Äôs non-linearity. By doing this, we can get a fair linear estimate of the natural scene brightness.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*UW_uGegQxkl17PlHU0f2WA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><p>This de-linearisation of the source allows to compensate for a non-linear display. Display technologies have advanced but the non-linearity still exists in most devices. This <em>de-linearisation</em> is known as gamma correction.</p><blockquote>Gamma corrected image = image ^ Œ≥</blockquote><p>If the input image is x, then what a display device of gamma=1.2 shows is x^(1.2). Therefore, the input image is encoded as x^(1/1.2) so that the monitor converts it to x^((1/1.2) x 1.2) which is equal to x, the original image captured by the camera.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*4k6-goBZ-RVLWjd9psEXAA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><p>For most displays these days, images have to be encoded by a gamma value of 0.45 (1/2.2) because of a gamma decoding of 2.2 by the displays.</p><p>Gamma encoding is performed over a range of [0,1]. So images first have to be normalised by dividing by 255 and then again multiplying by 255 after the gamma operation. Powers of greater than 1 yield darker images whereas powers of less than 1 yield brighter image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*grxUUm_ZH2CmaLs-ynH0EA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Comparison between gamma encoded images</figcaption></figure><p>HDR photography (or any photography) is quite complex in a way that we need to think of three important aspects ‚Äî</p><ul><li>How the actual scene is (ground truth / uncapped dynamic range)</li><li>How the camera captures (bracketing and then merging)</li><li>How it is displayed (tonemapping)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*wNoLL8Iz5IIR-pEb.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption><a href="https://www.cambridgeincolour.com/tutorials/dynamic-range.htm" data-href="https://www.cambridgeincolour.com/tutorials/dynamic-range.htm" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Source</a></figcaption></figure><p>In the context of HDR imaging, we have discussed the first two points. Now let‚Äôs look at how HDR images can be displayed.</p><h3 id="tonemapping">Tonemapping</h3><p>Most off the shelf display devices are incapable of delivering the wide uncapped range of HDR images. They expect the input source to be in the three-channel 24-bit (3x8) RGB format. Due to this reason, the wide dynamic range needs to be toned down to be able to accommodate it in the 0‚Äì255 range of RGB format. This can be done in several ways, some of which are-</p><ul><li>Conventional linear normalisation: This is the most basic way of bringing down the wide range of an HDR image.</li></ul><blockquote>tonemapped image = (img‚Ää‚Äî‚Ääimg.min()/img.max()‚Ää‚Äî‚Ääimg.min()) x 255</blockquote><ul><li>Reinhard tonemapping: This is one of the most commonly used tonemapping algorithm that was shared in this <a href="//www.cmap.polytechnique.fr/~peyre/cours/x2005signal/hdr_photographic.pdf" rel="noopener">paper</a>.</li></ul><blockquote>tonemapped image = img/(1+img) x 255</blockquote><ul><li>Drago tonemapping: This tonemapper is a perception-based one that compresses the dynamic range using logarithmic functions ‚Äúcomputed using different bases from the scene content‚Äù. The paper for this can be found <a href="//resources.mpi-inf.mpg.de/tmo/logmap/logmap.pdf" rel="noopener">here</a>.</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*0gbgFTFg_mlZRqdWa7b8qw.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"></figure><p>You asked for it</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*9Ss_5MkHidaiJcIU67vHUg.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? üì∑"><figcaption>Results</figcaption></figure><h3 id="ongoing-research-in-generating-hdr-content">Ongoing research in generating HDR content</h3><p>The conventional approach of generating HDR content is by merging multiple images captured at different exposures (bracketing). However, this approach is likely to create ghosting (blur) artifacts when there is movement between the frames. This has been solved by first aligning the neighbouring frame with the reference frame (middle frame) using something known as <em>optical flow</em>. That can be a topic for another blog post but for now we can think of it as a way to estimate the motion of objects (or pixels) that happens across frames by assigning a displacement vector to certain pixel positions.</p><p>There also has been work in generating HDR frames from singular LDR counterparts using Deep Learning. Neural networks are successfuly able to learn complex representations between the input and the output, and have thus performed quite well in learning the LDR to HDR mapping. These are some of the state-of-the-art methods for HDR image generation from a single image ‚Äî</p><ul><li>HDRCNN: <a href="//hdrv.org/hdrcnn/material/sga17_paper.pdf" rel="noopener">HDR image reconstruction from a single exposure using deep CNNs</a></li><li><a href="https://arxiv.org/pdf/1903.01277.pdf" rel="noopener">Deep Inverse Tone Mapping Using LDR Based Learning for Estimating HDR Images with Absolute Luminance</a></li><li><a href="https://arxiv.org/abs/1803.02266" rel="noopener">ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content</a></li></ul><p>Here are some state-of-the-art Deep Learning based methods for HDR image generation using multiple LDR images ‚Äî</p><ul><li><a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Deep High Dynamic Range Imaging of Dynamic Scenes</a></li><li>AHDRNet: <a href="https://arxiv.org/abs/1904.10293" rel="noopener">Attention-guided Network for Ghost-free High Dynamic Range Imaging</a></li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://towardsdatascience.com/paper-review-attention-guided-network-for-ghost-free-high-dynamic-range-imaging-4df2ec378e8"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Paper review‚Ää‚Äî‚ÄäAttention-guided Network for Ghost-free High Dynamic Range Imaging</div><div class="kg-bookmark-description">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at CVPR 2019‚Ä¶</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png" alt="HDR Imaging: What is an HDR image anyway? üì∑"><span class="kg-bookmark-author">Mukul Khanna</span><span class="kg-bookmark-publisher">Towards Data Science</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://miro.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" alt="HDR Imaging: What is an HDR image anyway? üì∑"></div></a></figure><h3 id="how-to-view-hdr-images">How to view HDR images</h3><p>HDR images are stored as luminance maps and not as conventional RGB images and thus can‚Äôt be viewed using common image viewing applications.</p><p>MacOS allows you to view .hdr and .exr files using the Preview and Finder app. You can also use the <a href="https://viewer.openhdr.org/" rel="noopener">OpenHDR</a> website to visualise such images.</p><hr><p>Thanks for reading.</p><p>For future blog posts, I would like to discuss about how operations can be performed on HDR images using Python, OpenCV and Numpy. I would also like to share the current scenario of research being done for HDR video generation.</p>]]></content:encoded></item><item><title><![CDATA[Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë]]></title><description><![CDATA[<figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" class="kg-image" alt><figcaption>Source: <a href="https://donggong1.github.io/ahdr" data-href="https://donggong1.github.io/ahdr" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet)</a></figcaption></figure><blockquote><em>‚ÄúYou just want attention, you don‚Äôt want my heart‚Äù‚Ää‚Äî‚ÄäCharlie Puth</em></blockquote><h3 id="about-the-paper">About the paper</h3><p><em>Attention-guided Network for Ghost-free High Dynamic Range Imaging</em> (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at</p>]]></description><link>/attention-guided-network-for-ghost-free-high-dynamic-range-imaging-cvpr-2019-paper-review/</link><guid isPermaLink="false">5f93a650f123610eb9e42a29</guid><category><![CDATA[1k+ views]]></category><category><![CDATA[Paper Review]]></category><category><![CDATA[HDR]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Tue, 24 Sep 2019 04:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1549545931-59bf067af9ab?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Source: <a href="https://donggong1.github.io/ahdr" data-href="https://donggong1.github.io/ahdr" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet)</a></figcaption></figure><blockquote><em>‚ÄúYou just want attention, you don‚Äôt want my heart‚Äù‚Ää‚Äî‚ÄäCharlie Puth</em></blockquote><h3 id="about-the-paper">About the paper</h3><img src="https://images.unsplash.com/photo-1549545931-59bf067af9ab?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><p><em>Attention-guided Network for Ghost-free High Dynamic Range Imaging</em> (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at CVPR 2019, and can be read <a href="https://arxiv.org/abs/1904.10293" rel="noopener">here</a>. The primary author, <a href="https://donggong1.github.io/" rel="noopener">Dong Gong</a> is a Postdoctoral researcher at The University of Adelaide. His interests include machine learning and optimization in Computer Vision.</p><blockquote><strong><em>Note</em></strong><em>: Image results, network representations, formulas and tables used in this blog post have all been sourced from the </em><a href="https://arxiv.org/abs/1904.10293" rel="noopener"><em>paper</em></a><em>.</em></blockquote><h3 id="el-problema">El problema</h3><p>For generation of an HDR image from multi-exposure bracketed LDR images, alignment of the LDR images is very important for dynamic scenes with in-frame motion. Misalignments not accounted for before merging causing ghosting (among other) artifacts. There have been several successful (almost) attempts at compensating this motion between frames by using Optical flow estimation. As it turns out, the shortcomings of flow based methods have not served the HDR cause well.</p><p>This can be seen in an attempt by <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>, where regardless of the accurate spatial reconstruction in saturated image regions, alignment artifacts can be observed for input frames with severe motion. This can be seen in the results provided by the authors of AHDRNet in the image below. Another attempt at HDR reconstruction that targets specifically at highly dynamic LDR bracketed input (<a href="https://arxiv.org/pdf/1711.08937.pdf" rel="noopener">Wu et. al</a>) claims to exploit the apparent prowess of CNN architectures in learning misalignments and compensating for the ghosting artifacts. Results presented below however show that there is scope for improvement.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*30oeBm3mjtt2UsDCSqu0gA.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><h3 id="attention-to-the-rescue">Attention to the rescue</h3><p>The authors propose to use attention mechanisms to solve this bi-faceted problem of reduction of alignment artifacts + accurate HDR reconstruction by using attention mechanisms. If you think about it, attention networks are just a very few Conv layers stacked together, followed (usually) by a sigmoid activation that allows the networks to focus on what‚Äôs important and pertinent to the application.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*kOhxdmePmBQfgrhhKMiwwA.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>Here, the attention networks are utilised to suppress the alignment artifacts and to focus on infusing out better-exposed image regions into the generated image by attending to the spatial dynamics of the bracketed images with respect to the reference image. Regions which correspond to the reference image are highlighted, whereas regions with severe motion and saturation are suppressed. We‚Äôll see how the attention information matrix is processed and implemented, in terms of the mathematics behind it, in some time.</p><p>The attention part of the network focuses on making decisions about which image regions contribute better to the accuracy of the network output. This is followed by a merge network that based on the attention output, tries to create HDR content from the LDR input. The better the attention mechanism, the better is the input to the merge network, thus allowing it to utilise information in the more relevant parts of the input. The merge network has been developed using dilated dense residual blocks that improve gradient-flow, hierarchical learning and convergence. The whole network is trained in an end-to-end fashion and therefore both subnetworks mutually influence each other, and learn together.</p><h3 id="implementation">Implementation</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*IJfbHMvhiFWC5yQe6PCxxg.jpeg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Overview</figcaption></figure><h4 id="preprocessing">Preprocessing</h4><p>The non-linear LDR input (<em>I1, I2 , I3</em>) is transferred to the linear domain by applying an inverse CRF (gamma correction here) and dividing by their corresponding exposure times.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*iy2Do0TiEa70L4ZZOrUU1A.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Œ≥ =&nbsp;2.2</figcaption></figure><p>Both the linear and non-linear input (<em>Ii, Hi</em>) are concatenated along the channel dimensions to form <em>Xi</em>. <em>X1</em> , <em>X2</em> and <em>X3</em> are fed to the network to generate the corresponding HDR output.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*MgAQuh0sfypm-2aEUM0g6A.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption><em class="markup--em markup--figure-em">H</em> is the generated image, <em class="markup--em markup--figure-em">f</em> (.) represents the AHDRNet network and <em class="markup--em markup--figure-em">Œ∏</em> the network parameters</figcaption></figure><p>The network performs better when it has the linearised input information at its disposal. This has been observed and utilised in <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a> as well as <a href="https://arxiv.org/pdf/1711.08937.pdf" rel="noopener">Wu et. al</a>.</p><h3 id="architecture">Architecture</h3><p>The whole network comprises of two sub-networks‚Ää‚Äî‚Ääattention network and merging network.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*FHnHzLbQz58BHcRMFhOd1w.jpeg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Attention network</figcaption></figure><p>The attention network, as discussed above helps in avoiding alignment artifacts by highlighting and using information from regions in the neighbouring images (non-reference images) that correspond to the reference image. It does so in the following way.</p><p>Attention is not extracted from and applied directly to the concatenated image pairs. First the <em>Xi</em> s are passed through a Conv layer to extract a 64 channel feature map <em>Zi</em>.</p><p>Then, the reference feature map (<em>Z2</em> or <em>Zr</em>) along with a neighbouring image feature map (<em>Z1</em> and <em>Z3</em>) is fed into the attention module that generates an attention map to mark the important regions in the non-reference feature map with reference to <em>Zr</em>.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*TeylpymAABwM2B6X.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>This is done for both the pairs‚Ää‚Äî‚Ää(<em>Z1</em> , <em>Z2</em>) and (<em>Z3</em> , <em>Z2</em>). This can be seen clearly in the above network representation.</p><p>Before we get into what the attention module does, let‚Äôs see what to do with the attention map that is generated. The attention map generated is essentially a 64-channel matrix that contains values between [0,1]. This matrix serves as a kind of a weight-matrix, in which each element represents the importance of a corresponding element in the feature matrix of the neighbour image, with reference to <em>Z2</em>. This is implemented by using the attention map generated from (<em>Z1</em> , <em>Z2</em>) by doing an element wise multiplication of the attention map and <em>Z1</em> to get <em>Z‚Äô1</em>.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*AyY92BJFOzyEAatL.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>This operation results in important features (where attention is closer to 1) in <em>Z1</em> getting higher numerical values and correspondingly lower values for less important features. This manifests in only important image regions from <em>Z1</em> going ahead in the network to contribute to the final HDR output. The same thing happens between (<em>Z3</em> , <em>Z2</em>) to get <em>Z‚Äô3</em>.</p><p>Now that we have all the input pieces most relevant to construct the HDR image, we concatenate these along the channel dimension as below -</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*oulHopStLvaePUth.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><h4 id="attention-module">Attention module</h4><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*VJ5ooOqafNOC5v7O.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>Let‚Äôs see how these attention maps are generated. The attention module used in this paper comprises of 2 Conv2d layers that output a 64-channel matrix, followed by a ReLU and a sigmoid activation respectively. It takes as input a concatenated feature vector of neighbouring and reference image (2 x 3 = 6 channels). The sigmoid activation, in the end, is used to contain the output in a [0,1] range.</p><h4 id="attention-results">Attention results</h4><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*op6pjaA5-eo-eecZ.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Attention map examples from the paper. ( a ) to ( c )‚Äî Alignment attention; ( d ) to ( f )‚Ää‚Äî‚ÄäExposure attention</figcaption></figure><p>In ( a ) to ( c ), it can be observed from the results above how regions with motion discrepancies in non-reference images are suppressed (darker blue region) whereas regions that have correspondances with the reference image are highlighted (brighter blue-ish green). In ( d ) to ( f ), the regions that are better exposed in the neighbouring frames are highlighted and saturated regions are suppressed.</p><h3 id="merging-network">Merging network</h3><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*nq0yC0HIakfLZcdZ.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>The concatenated feature map (<em>Zs</em>) is given as input to the merging network. The merging network used by the authors is the residual dense network proposed in <a href="https://arxiv.org/abs/1802.08797" rel="noopener">Zhang et. al</a>. Instead of the conventional Conv operations, the authors have used dilated convolutions to propagate a larger receptive field, thus calling it a Dilated Residual Dense Block (DRDB). There are 3 such blocks in the merging network that consist of dense concatenation based skip-connections and residual connections that have been proved quite effective for CNNs in solving gradient vanishing, allowing better backpropagation, hierarchical learning and therefore aiding and improving convergence performance. In the proposed AHDRNet network, each DRDB consists of 6 Conv layers and a growth rate of 32.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*1ZrNm30PPxTHo9Iw.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>DRDB</figcaption></figure><p>The authors have also employed local and global residual skip connections that bypass low level features to higher level ones. Local residual learning is implemented within the DRDBs whereas global residual learning is for transferring the shallow feature maps containing <em>pure</em> information from the reference image to the latter stages. This, and other network specifications can be observed in the merging network diagram.</p><h3 id="loss-functions">Loss functions</h3><p>Just like <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>, the loss is calculated between <em>Œº</em>-law tonemapped generated and tonemapped ground truth images. <em>Œº</em> has been set to 5000 for all the experiments. The <em>Œº</em>-law can be defined as -</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*W_ebdoq-1Dus37Gi.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption><em class="markup--em markup--figure-em">Œº</em>-law</figcaption></figure><p>An L1 loss has been used for the same. Quantitative comparisons PSNR and HDR-VDP-2 scores presented in the paper convey that L1 loss is better at reconstructing finer details as compared to an L2 loss.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*In81bfClnA_qHrlV.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><h4 id="implementation-specifications">Implementation specifications</h4><p>The architecture was implemented using PyTorch. The specifications and hyper-parameters are -</p><ul><li>Weight initialisation: Xavier</li><li>Optimizer: ADAM</li><li>Learning rate: 1 x 10‚Äì5</li><li>Batch size: 8</li><li>GPU: NVIDIA GeForce 1080 Ti</li><li>Inference time for 1 image (1500x1000): 0.32 sec</li></ul><h4 id="results">Results</h4><p>The networks were trained and tested on the datasets provided by <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>. The authors have provided quantitative and qualitative comparisons between several variants of the network, in terms of PSNR and HDR-VDP-2 scores.</p><ul><li>AHDRNet‚Ää‚Äî‚ÄäThe full model of the AHDRNet.</li><li>DRDB-Net (i.e. AHDRNet w/o attention)</li><li>A-RDB-Net (i.e. AHDRNet w/o dilation)</li><li>RDB-Net (i.e. AHDRNet w/o attention and dilation)</li><li>RB-Net (i.e. AHDRNet w/o attention, dilation and dense connections). DRDBs replaced by RBs.</li><li>Deep-RB-Net. More RBs are used.</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*EUnWQxWutnMEJg6i.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>The results show how each component of the AHDRNet is important to the efficacy of the whole network i.e. attention is important, dilated convolutions are important, dense connections are important and residual learning is important.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*WC-uKJVWhFLumxbb.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"><figcaption>Visual results, from the&nbsp;paper</figcaption></figure><p>Comparison with state-of-the-art</p><p>Comparisons with current state-of-the-art approaches (learning based and non-learning based) reveal how AHDRNet beats existing approaches. The closest competitor is obviously <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>‚Äôs implementation that comes only second to AHDRNet. The authors have also provided the results of a variant of AHDRNet that uses Optical-flow aligned images (AHDRNet + OF).</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*iWVJeN5urpZVz9Ka.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><p>The visual results show the efficacy of the network in infusing meticulous details in the generated HDR output without giving rise to any alignment artifacts even in cases of severe motion. Here are some of the results, taken from the paper -</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*muRv_BXdBEHgJOxd.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*H1TAnwS6l0whMraX.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) üìë"></figure><h3 id="conclusion">Conclusion</h3><p>AHDRNet is the first attention-based approach to solving the problem of HDR image generation. The finesse of attention mechanisms has been utilised to align the input LDR images. Previous attempts at image alignment have used Optical flow-based methods, which have some inaccuracies and do not perform well for severe motion between frames. An attention-based approach however performs exceedingly well in terms of HDR reconstruction as well as in removing alignment artifacts. Extensive experiment reveal how AHDRNet supersedes existing approaches qualitatively and quantitatively, and that it has become the new state-of-the-art in HDR image generation.</p><h3 id="references">References</h3><ul><li>Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, Yanning Zhang, <a href="https://arxiv.org/abs/1904.10293" rel="noopener">Attention-guided Network for Ghost-free High Dynamic Range Imaging</a> (2019), CVPR 2019</li><li>N. K. Kalantari and R. Ramamoorthi, <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Deep high dynamic rangeimaging of dynamic scenes</a> (2017), ACM Trans. Graph</li><li>Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang, <a href="https://arxiv.org/abs/1711.08937" rel="noopener">Deep high dynamic range imaging with large foreground motions</a> (2018), European Conference on Computer Vision (ECCV), September 2018</li></ul>]]></content:encoded></item></channel></rss>