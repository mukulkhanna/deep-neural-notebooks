<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Paper Review - Deep Neural Notebooks]]></title><description><![CDATA[Research, Insights & Stories]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Paper Review - Deep Neural Notebooks</title><link>http://localhost:2368/</link></image><generator>Ghost 3.21</generator><lastBuildDate>Fri, 30 Oct 2020 23:33:10 GMT</lastBuildDate><atom:link href="http://localhost:2368/tag/paper-review/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑]]></title><description><![CDATA[<p>CVPR 2017, Best Paper Award winner</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt><figcaption>Dense connections</figcaption></figure><blockquote><strong>“Simple models and a lot of data trump more elaborate models based on less data. “ — </strong>Peter Norvig</blockquote><h3 id="about-the-paper">About the paper</h3><p>‘<strong>Densely Connected Convolutional Networks</strong>’ received the <strong>Best Paper Award</strong> at the IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>) <strong>2017</strong>. The</p>]]></description><link>http://localhost:2368/densenet-densely-connected-convolutional-networks/</link><guid isPermaLink="false">5f93aa97f123610eb9e42a54</guid><category><![CDATA[8k+ views]]></category><category><![CDATA[Paper Review]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Sun, 10 Nov 2019 04:23:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/1_TeHVqikNc68QC98Vm9M98Q.gif" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2020/10/1_TeHVqikNc68QC98Vm9M98Q.gif" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><p>CVPR 2017, Best Paper Award winner</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Dense connections</figcaption></figure><blockquote><strong>“Simple models and a lot of data trump more elaborate models based on less data. “ — </strong>Peter Norvig</blockquote><h3 id="about-the-paper">About the paper</h3><p>‘<strong>Densely Connected Convolutional Networks</strong>’ received the <strong>Best Paper Award</strong> at the IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>) <strong>2017</strong>. The paper can be read <a href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener">here</a>.</p><p>The primary author, <a href="http://www.gaohuang.net/" rel="noopener"><strong>Gao Huang</strong></a> has been a Postdoctoral Fellow at Cornell University and is currently working at Tsinghua University as an Assistant Professor. His research focuses on deep learning for computer vision.</p><h3 id="how-i-came-across-the-paper">How I came across the paper?</h3><p>I came across this paper while researching for neural network implementations that were focused on improving image quality (in terms of resolution or dynamic range) by reconstruction. Although this paper demonstrates the prowess of the architecture in image classification, the idea of dense connections has inspired optimisations in many other deep learning domains like image super-resolution, image segmentation, medical diagnosis etc.</p><h3 id="key-contributions-of-the-densenet-architecture">Key contributions of the DenseNet architecture</h3><ul><li>Alleviates <strong>vanishing gradient problem</strong></li><li>Stronger <strong>feature propagation</strong></li><li><strong>Feature reuse</strong></li><li><strong>Reduced parameter</strong> count</li></ul><blockquote><strong>Before you read: </strong><br>Understanding this post requires a basic understanding of deep learning concepts.</blockquote><h3 id="paper-review">Paper review</h3><p>The paper starts with talking about the <strong>vanishing gradient problem — </strong>about how, as networks get deeper, gradients aren’t back-propagated sufficiently to the initial layers of the network. The gradients keep getting smaller as they move backwards into the network and as a result, the initial layers lose their capacity to learn the basic low-level features.</p><p>Several architectures have been developed to solve this problem. These include — ResNets, Highway Networks, Fractal Nets, Stochastic depth networks.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*MwxLN83uT8bgd4nGiR6JZQ.jpeg" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"></figure><p>Regardless of the architectural designs of these networks, they all try to create channels for information to flow between the initial layers and the final layers. DenseNets, with the same objective, create paths between the layers of the network.</p><h3 id="related-works">Related works</h3><ul><li>Highway networks (one of the first attempts at making training easy for deeper models)</li><li>ResNet (Bypassing connections by summation using identity mappings)</li><li>Stochastic depth (dropping layers randomly during training)</li><li>GoogLeNet (inception module — increasing network width)</li><li>FractalNet</li><li>Network in Network (NIN)</li><li>Deeply Supervised Network (DSN)</li><li>Ladder Networks</li><li>Deeply-Fused Nets (DFNs)</li></ul><h3 id="dense-connections">Dense connections</h3><p>Following the feed-forward nature of the network, each layer in a dense block receives feature maps from all the preceding layers, and passes its output to all subsequent layers. Feature maps received from other layers are fused through <strong>concatenation</strong>, and not through summation (like in ResNets).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*StSq7XyHcuxUOfWuuALkdw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Concatenation of feature&nbsp;maps</figcaption></figure><p>These connections form a dense circuit of pathways that allow <strong>better gradient-flow</strong>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Dense connections</figcaption></figure><blockquote>Each layer has direct access to the gradients of the loss function and the original input signal.</blockquote><p>Because of these dense connections, the model requires fewer layers, as there is no need to learn redundant feature maps, allowing the <strong>collective knowledge </strong>(features learnt collectively by the network) to be reused. The proposed architecture has narrow layers, which provide state-of-the-art results for as low as 12 channel feature maps. Fewer and narrower layers means that the model has <strong>fewer parameters</strong> to learn, making them easier to train. The authors also talk about the importance of variation in input of layers as a result of concatenated feature maps, which prevents the model from over-fitting the training data.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*04TJTANujOsauo3foe0zbw.jpeg" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"></figure><p>Many variants of the DenseNet model have been presented in the paper. I have opted to explain the concepts with their standard network (DenseNet-121).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*rkra9kVPl754-vjRGsOqvw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Some of the variants of the DenseNet architecture</figcaption></figure><h3 id="composite-function">Composite function</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*qA2rLVBRB-wZoI3nAEMdkA.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Composite function</figcaption></figure><p>*Each CONV block in the network representations in the paper (and in the blog) corresponds to an operation of —</p><p><strong>BatchNorm→ReLU→Conv*</strong></p><h3 id="dense-block">Dense block</h3><p>The concept of dense connections has been portrayed in dense blocks. A dense block comprises <em>n </em>dense layers. These dense layers are connected using a dense circuitry such that each dense layer receives feature maps from all preceding layers and passes it’s feature maps to all subsequent layers. The dimensions of the features (width, height) stay the same in a dense block.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*ZfrliiHwn_L4kKcO61Oxgw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><strong class="markup--strong markup--figure-strong">Dense block (DB) with six Dense Layers&nbsp;(DL)</strong></figcaption></figure><h3 id="dense-layer">Dense layer</h3><p>Each dense-layer consists of 2 convolutional operations -</p><ul><li><strong>1 X 1 CONV </strong>(conventional conv operation for extracting features)</li><li><strong>3 X 3 CONV </strong>(bringing down the feature depth/channel count)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*cRXqccOxYkZbWpfmyXzjog.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><strong class="markup--strong markup--figure-strong">Dense layer of&nbsp;DB-1</strong></figcaption></figure><p>The DenseNet-121 comprises of 6 such dense layers in a dense block. The depth of the output of each dense-layer is equal to the growth rate of the dense block.</p><h3 id="growth-rate-k-">Growth rate (k)</h3><p>This is a term you’ll come across a lot in the paper. It is basically the number of channels output by a dense-layer (<em>1x1 conv → 3x3 conv</em>). The authors have used a value of <em>k = 32</em> for the experiments. This means that the number of features received by a dense layer ( <em>l </em>) from it’s preceding dense layer ( <em>l-1</em> ) is 32. This is referred to as the growth rate because after each layer, 32 channel features are concatenated and fed as input to the next layer.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*NIQenf9KTillNhYfuySfMw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Dense block with channel count (C) of features entering and exiting the&nbsp;layers</figcaption></figure><h3 id="transition-layer">Transition layer</h3><p>At the end of each dense block, the number of feature-maps accumulates to a value of — <em>input features + (number of dense layers x growth rate). </em>So for 64 channel features entering a dense block of 6 dense-layers of growth rate 32, the number of channels accumulated at the end of the block will be — <br>64 + (6 x 32) = 256. To bring down this channel count, a <strong>transition layer </strong>(or block) is added between two dense blocks. The transition layer consists of -</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*LoFEV57u5kCjsqZRX7EAKw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><strong class="markup--strong markup--figure-strong">Transition layer/block</strong></figcaption></figure><ul><li><strong>1 X 1 CONV</strong> operation</li><li><strong>2 X 2 AVG POOL </strong>operation</li></ul><p>The <strong>1 X 1 CONV </strong>operation reduces the channel count to half.<br> The <strong>2 X 2 AVG POOL</strong> layer is responsible for downsampling the features in terms of the width and height.</p><h3 id="full-network">Full network</h3><p>As can be seen in the diagram below, the authors have chosen different number of dense layers for each of the three dense block.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*CE11_lfEz00aoOjLiw5sdw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><strong class="markup--strong markup--figure-strong">Full DenseNet architecture</strong></figcaption></figure><h3 id="comparison-with-densenet">Comparison with DenseNet</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*cXAoIC_ig5R8nE4hKj2OeQ.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>ResNet DenseNet comparison</figcaption></figure><p>We can see that even with a reduced parameter count, the DenseNet model has a significantly lower validation error for the ResNet model with the same number of parameters. These experiments were carried out on both the models with hyper-parameters that suited ResNet better. The authors claim that DenseNet would perform better after extensive hyper-parameter searches.</p><blockquote>DenseNet-201 with 20M parameters model yields similar validation error as a 101-layer ResNet with more than 40M parameters.</blockquote><h3 id="inspecting-the-code">Inspecting the code</h3><p>I believe that going through the code makes it easier to understand the implementations of such architectures. Research papers (in the context of Deep Learning) can be difficult to understand because they are more about what drives the design decisions of a neural network. Inspecting the code (usually the network/model code) can reduce this complexity because sometimes it’s just the implementation that we are interested in. Some people prefer first seeing the implementation and then trying to figure out the reasoning behind the design decisions of the network. Regardless, reading the code, before or after, always helps.</p><p>The code of the DenseNet implementation can be found <a href="https://github.com/liuzhuang13/DenseNet" rel="noopener">here</a>. Since I am more comfortable with PyTorch, I’ll try to explain the PyTorch implementation of the model which can be found <a href="https://github.com/gpleiss/efficient_densenet_pytorch" rel="noopener">here</a>. The most important file would be <strong>models/densenet.py</strong>, that hold the network architecture for DenseNet.</p><p>The code has been divided into these classes where each type of block is represented by a class.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*RgsliXFElnLQ5uh5ZCDBAg.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Class hierarchy in&nbsp;code</figcaption></figure><h3 id="dense-layer-1">Dense layer</h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/1200979cc91e54f2e638872fd4680560.js"></script>
<!--kg-card-end: html--><p>The<strong> _DenseLayer </strong>class can be used to initialise the constituent operations of a dense layer —</p><p><strong>BatchNorm → ReLU → Conv (1X1) → BatchNom → ReLU → Conv (3X3)</strong></p><p>The <strong>_bn_function_factory()</strong> function is responsible for concatenating the output of the previous layers to the current layer.</p><h3 id="denseblock"><strong>DenseBlock</strong></h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/0783bc22b0b8f826ef92e1e7455c3075.js"></script><!--kg-card-end: html--><p>The _<strong>DenseBlock </strong>class houses a certain number of <strong>_DenseLayer</strong>s (<em>num_layers</em>).<br>This class is initialised from the <strong>DenseNet </strong>class depending on the number of dense blocks used in the network.</p><h3 id="transition-block">Transition Block</h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/86feacfbdbbc4ad841adac0e00e4de75.js"></script><!--kg-card-end: html--><h3 id="densenet"><strong>DenseNet</strong></h3><p>Since this part of the code is a little too big to fit in this blog, I’ll just be using a part of the code that should help in getting the gist of the network.</p><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/2238b561aca1c29878310fe322f1ba54.js"></script><!--kg-card-end: html--><p>I found this image online and it has helped me understand the network better.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*vIZhPImFr9Gjpx6ZB7IOJg.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><a href="https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a" data-href="https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Source</a></figcaption></figure><h3 id="other-works-inspired-from-this-paper">Other works inspired from this paper</h3><ul><li><a href="https://arxiv.org/abs/1802.08797" rel="noopener">Residual Dense Network for Image Super-Resolution</a> (2018)</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*6NS5NPZoU3iQXIJOu6jruQ.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"></figure><ul><li><a href="http://www.statnlp.org/research/lg/zhijiang_zhangyan19tacl-gcn.pdf" rel="noopener">Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning</a> (2019)</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*pa45Gjp8H1D9NnXJE6Vypw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"></figure><h3 id="conclusion">Conclusion</h3><p>DenseNet is a network that portrays the importance of having dense connections in a network using dense blocks. This helps in feature-reuse, better gradient flow, reduced parameter count and better transmission of features across the network. Such an implementation can help in training deeper networks using less computational resources and with better results.<br></p>]]></content:encoded></item><item><title><![CDATA[Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑]]></title><description><![CDATA[<figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" class="kg-image" alt><figcaption>Source: <a href="https://donggong1.github.io/ahdr" data-href="https://donggong1.github.io/ahdr" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet)</a></figcaption></figure><blockquote><em>“You just want attention, you don’t want my heart” — Charlie Puth</em></blockquote><h3 id="about-the-paper">About the paper</h3><p><em>Attention-guided Network for Ghost-free High Dynamic Range Imaging</em> (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at</p>]]></description><link>http://localhost:2368/attention-guided-network-for-ghost-free-high-dynamic-range-imaging-cvpr-2019-paper-review/</link><guid isPermaLink="false">5f93a650f123610eb9e42a29</guid><category><![CDATA[1k+ views]]></category><category><![CDATA[Paper Review]]></category><category><![CDATA[HDR]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Tue, 24 Sep 2019 04:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1549545931-59bf067af9ab?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption>Source: <a href="https://donggong1.github.io/ahdr" data-href="https://donggong1.github.io/ahdr" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet)</a></figcaption></figure><blockquote><em>“You just want attention, you don’t want my heart” — Charlie Puth</em></blockquote><h3 id="about-the-paper">About the paper</h3><img src="https://images.unsplash.com/photo-1549545931-59bf067af9ab?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><p><em>Attention-guided Network for Ghost-free High Dynamic Range Imaging</em> (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at CVPR 2019, and can be read <a href="https://arxiv.org/abs/1904.10293" rel="noopener">here</a>. The primary author, <a href="https://donggong1.github.io/" rel="noopener">Dong Gong</a> is a Postdoctoral researcher at The University of Adelaide. His interests include machine learning and optimization in Computer Vision.</p><blockquote><strong><em>Note</em></strong><em>: Image results, network representations, formulas and tables used in this blog post have all been sourced from the </em><a href="https://arxiv.org/abs/1904.10293" rel="noopener"><em>paper</em></a><em>.</em></blockquote><h3 id="el-problema">El problema</h3><p>For generation of an HDR image from multi-exposure bracketed LDR images, alignment of the LDR images is very important for dynamic scenes with in-frame motion. Misalignments not accounted for before merging causing ghosting (among other) artifacts. There have been several successful (almost) attempts at compensating this motion between frames by using Optical flow estimation. As it turns out, the shortcomings of flow based methods have not served the HDR cause well.</p><p>This can be seen in an attempt by <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>, where regardless of the accurate spatial reconstruction in saturated image regions, alignment artifacts can be observed for input frames with severe motion. This can be seen in the results provided by the authors of AHDRNet in the image below. Another attempt at HDR reconstruction that targets specifically at highly dynamic LDR bracketed input (<a href="https://arxiv.org/pdf/1711.08937.pdf" rel="noopener">Wu et. al</a>) claims to exploit the apparent prowess of CNN architectures in learning misalignments and compensating for the ghosting artifacts. Results presented below however show that there is scope for improvement.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*30oeBm3mjtt2UsDCSqu0gA.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><h3 id="attention-to-the-rescue">Attention to the rescue</h3><p>The authors propose to use attention mechanisms to solve this bi-faceted problem of reduction of alignment artifacts + accurate HDR reconstruction by using attention mechanisms. If you think about it, attention networks are just a very few Conv layers stacked together, followed (usually) by a sigmoid activation that allows the networks to focus on what’s important and pertinent to the application.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*kOhxdmePmBQfgrhhKMiwwA.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><p>Here, the attention networks are utilised to suppress the alignment artifacts and to focus on infusing out better-exposed image regions into the generated image by attending to the spatial dynamics of the bracketed images with respect to the reference image. Regions which correspond to the reference image are highlighted, whereas regions with severe motion and saturation are suppressed. We’ll see how the attention information matrix is processed and implemented, in terms of the mathematics behind it, in some time.</p><p>The attention part of the network focuses on making decisions about which image regions contribute better to the accuracy of the network output. This is followed by a merge network that based on the attention output, tries to create HDR content from the LDR input. The better the attention mechanism, the better is the input to the merge network, thus allowing it to utilise information in the more relevant parts of the input. The merge network has been developed using dilated dense residual blocks that improve gradient-flow, hierarchical learning and convergence. The whole network is trained in an end-to-end fashion and therefore both subnetworks mutually influence each other, and learn together.</p><h3 id="implementation">Implementation</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*IJfbHMvhiFWC5yQe6PCxxg.jpeg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption>Overview</figcaption></figure><h4 id="preprocessing">Preprocessing</h4><p>The non-linear LDR input (<em>I1, I2 , I3</em>) is transferred to the linear domain by applying an inverse CRF (gamma correction here) and dividing by their corresponding exposure times.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*iy2Do0TiEa70L4ZZOrUU1A.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption>γ =&nbsp;2.2</figcaption></figure><p>Both the linear and non-linear input (<em>Ii, Hi</em>) are concatenated along the channel dimensions to form <em>Xi</em>. <em>X1</em> , <em>X2</em> and <em>X3</em> are fed to the network to generate the corresponding HDR output.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*MgAQuh0sfypm-2aEUM0g6A.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption><em class="markup--em markup--figure-em">H</em> is the generated image, <em class="markup--em markup--figure-em">f</em> (.) represents the AHDRNet network and <em class="markup--em markup--figure-em">θ</em> the network parameters</figcaption></figure><p>The network performs better when it has the linearised input information at its disposal. This has been observed and utilised in <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a> as well as <a href="https://arxiv.org/pdf/1711.08937.pdf" rel="noopener">Wu et. al</a>.</p><h3 id="architecture">Architecture</h3><p>The whole network comprises of two sub-networks — attention network and merging network.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*FHnHzLbQz58BHcRMFhOd1w.jpeg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption>Attention network</figcaption></figure><p>The attention network, as discussed above helps in avoiding alignment artifacts by highlighting and using information from regions in the neighbouring images (non-reference images) that correspond to the reference image. It does so in the following way.</p><p>Attention is not extracted from and applied directly to the concatenated image pairs. First the <em>Xi</em> s are passed through a Conv layer to extract a 64 channel feature map <em>Zi</em>.</p><p>Then, the reference feature map (<em>Z2</em> or <em>Zr</em>) along with a neighbouring image feature map (<em>Z1</em> and <em>Z3</em>) is fed into the attention module that generates an attention map to mark the important regions in the non-reference feature map with reference to <em>Zr</em>.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*TeylpymAABwM2B6X.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><p>This is done for both the pairs — (<em>Z1</em> , <em>Z2</em>) and (<em>Z3</em> , <em>Z2</em>). This can be seen clearly in the above network representation.</p><p>Before we get into what the attention module does, let’s see what to do with the attention map that is generated. The attention map generated is essentially a 64-channel matrix that contains values between [0,1]. This matrix serves as a kind of a weight-matrix, in which each element represents the importance of a corresponding element in the feature matrix of the neighbour image, with reference to <em>Z2</em>. This is implemented by using the attention map generated from (<em>Z1</em> , <em>Z2</em>) by doing an element wise multiplication of the attention map and <em>Z1</em> to get <em>Z’1</em>.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*AyY92BJFOzyEAatL.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><p>This operation results in important features (where attention is closer to 1) in <em>Z1</em> getting higher numerical values and correspondingly lower values for less important features. This manifests in only important image regions from <em>Z1</em> going ahead in the network to contribute to the final HDR output. The same thing happens between (<em>Z3</em> , <em>Z2</em>) to get <em>Z’3</em>.</p><p>Now that we have all the input pieces most relevant to construct the HDR image, we concatenate these along the channel dimension as below -</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*oulHopStLvaePUth.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><h4 id="attention-module">Attention module</h4><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*VJ5ooOqafNOC5v7O.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><p>Let’s see how these attention maps are generated. The attention module used in this paper comprises of 2 Conv2d layers that output a 64-channel matrix, followed by a ReLU and a sigmoid activation respectively. It takes as input a concatenated feature vector of neighbouring and reference image (2 x 3 = 6 channels). The sigmoid activation, in the end, is used to contain the output in a [0,1] range.</p><h4 id="attention-results">Attention results</h4><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*op6pjaA5-eo-eecZ.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption>Attention map examples from the paper. ( a ) to ( c )— Alignment attention; ( d ) to ( f ) — Exposure attention</figcaption></figure><p>In ( a ) to ( c ), it can be observed from the results above how regions with motion discrepancies in non-reference images are suppressed (darker blue region) whereas regions that have correspondances with the reference image are highlighted (brighter blue-ish green). In ( d ) to ( f ), the regions that are better exposed in the neighbouring frames are highlighted and saturated regions are suppressed.</p><h3 id="merging-network">Merging network</h3><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*nq0yC0HIakfLZcdZ.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><p>The concatenated feature map (<em>Zs</em>) is given as input to the merging network. The merging network used by the authors is the residual dense network proposed in <a href="https://arxiv.org/abs/1802.08797" rel="noopener">Zhang et. al</a>. Instead of the conventional Conv operations, the authors have used dilated convolutions to propagate a larger receptive field, thus calling it a Dilated Residual Dense Block (DRDB). There are 3 such blocks in the merging network that consist of dense concatenation based skip-connections and residual connections that have been proved quite effective for CNNs in solving gradient vanishing, allowing better backpropagation, hierarchical learning and therefore aiding and improving convergence performance. In the proposed AHDRNet network, each DRDB consists of 6 Conv layers and a growth rate of 32.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*1ZrNm30PPxTHo9Iw.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption>DRDB</figcaption></figure><p>The authors have also employed local and global residual skip connections that bypass low level features to higher level ones. Local residual learning is implemented within the DRDBs whereas global residual learning is for transferring the shallow feature maps containing <em>pure</em> information from the reference image to the latter stages. This, and other network specifications can be observed in the merging network diagram.</p><h3 id="loss-functions">Loss functions</h3><p>Just like <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>, the loss is calculated between <em>μ</em>-law tonemapped generated and tonemapped ground truth images. <em>μ</em> has been set to 5000 for all the experiments. The <em>μ</em>-law can be defined as -</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*W_ebdoq-1Dus37Gi.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption><em class="markup--em markup--figure-em">μ</em>-law</figcaption></figure><p>An L1 loss has been used for the same. Quantitative comparisons PSNR and HDR-VDP-2 scores presented in the paper convey that L1 loss is better at reconstructing finer details as compared to an L2 loss.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*In81bfClnA_qHrlV.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><h4 id="implementation-specifications">Implementation specifications</h4><p>The architecture was implemented using PyTorch. The specifications and hyper-parameters are -</p><ul><li>Weight initialisation: Xavier</li><li>Optimizer: ADAM</li><li>Learning rate: 1 x 10–5</li><li>Batch size: 8</li><li>GPU: NVIDIA GeForce 1080 Ti</li><li>Inference time for 1 image (1500x1000): 0.32 sec</li></ul><h4 id="results">Results</h4><p>The networks were trained and tested on the datasets provided by <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>. The authors have provided quantitative and qualitative comparisons between several variants of the network, in terms of PSNR and HDR-VDP-2 scores.</p><ul><li>AHDRNet — The full model of the AHDRNet.</li><li>DRDB-Net (i.e. AHDRNet w/o attention)</li><li>A-RDB-Net (i.e. AHDRNet w/o dilation)</li><li>RDB-Net (i.e. AHDRNet w/o attention and dilation)</li><li>RB-Net (i.e. AHDRNet w/o attention, dilation and dense connections). DRDBs replaced by RBs.</li><li>Deep-RB-Net. More RBs are used.</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*EUnWQxWutnMEJg6i.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><p>The results show how each component of the AHDRNet is important to the efficacy of the whole network i.e. attention is important, dilated convolutions are important, dense connections are important and residual learning is important.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*WC-uKJVWhFLumxbb.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"><figcaption>Visual results, from the&nbsp;paper</figcaption></figure><p>Comparison with state-of-the-art</p><p>Comparisons with current state-of-the-art approaches (learning based and non-learning based) reveal how AHDRNet beats existing approaches. The closest competitor is obviously <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Kalantari et. al</a>’s implementation that comes only second to AHDRNet. The authors have also provided the results of a variant of AHDRNet that uses Optical-flow aligned images (AHDRNet + OF).</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*iWVJeN5urpZVz9Ka.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><p>The visual results show the efficacy of the network in infusing meticulous details in the generated HDR output without giving rise to any alignment artifacts even in cases of severe motion. Here are some of the results, taken from the paper -</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*muRv_BXdBEHgJOxd.jpg" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/0*H1TAnwS6l0whMraX.png" class="kg-image" alt="Attention-guided Network for Ghost-free High Dynamic Range Imaging (CVPR 2019) 📑"></figure><h3 id="conclusion">Conclusion</h3><p>AHDRNet is the first attention-based approach to solving the problem of HDR image generation. The finesse of attention mechanisms has been utilised to align the input LDR images. Previous attempts at image alignment have used Optical flow-based methods, which have some inaccuracies and do not perform well for severe motion between frames. An attention-based approach however performs exceedingly well in terms of HDR reconstruction as well as in removing alignment artifacts. Extensive experiment reveal how AHDRNet supersedes existing approaches qualitatively and quantitatively, and that it has become the new state-of-the-art in HDR image generation.</p><h3 id="references">References</h3><ul><li>Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, Yanning Zhang, <a href="https://arxiv.org/abs/1904.10293" rel="noopener">Attention-guided Network for Ghost-free High Dynamic Range Imaging</a> (2019), CVPR 2019</li><li>N. K. Kalantari and R. Ramamoorthi, <a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Deep high dynamic rangeimaging of dynamic scenes</a> (2017), ACM Trans. Graph</li><li>Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang, <a href="https://arxiv.org/abs/1711.08937" rel="noopener">Deep high dynamic range imaging with large foreground motions</a> (2018), European Conference on Computer Vision (ECCV), September 2018</li></ul>]]></content:encoded></item></channel></rss>