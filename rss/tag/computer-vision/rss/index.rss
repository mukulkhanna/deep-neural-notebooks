<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Computer Vision - Deep Neural Notebooks]]></title><description><![CDATA[Research, Insights & Stories]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Computer Vision - Deep Neural Notebooks</title><link>http://localhost:2368/</link></image><generator>Ghost 3.21</generator><lastBuildDate>Thu, 19 Nov 2020 12:28:56 GMT</lastBuildDate><atom:link href="http://localhost:2368/tag/computer-vision/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[DNN 9: NVIDIA's AI Co-Pilot , Computer Vision & ML Inside The Car // Shalini De Mello]]></title><description><![CDATA[<figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-05-at-6.24.33-PM.png" class="kg-image" alt srcset="http://localhost:2368/content/images/size/w600/2020/10/Screenshot-2020-10-05-at-6.24.33-PM.png 600w, http://localhost:2368/content/images/size/w1000/2020/10/Screenshot-2020-10-05-at-6.24.33-PM.png 1000w, http://localhost:2368/content/images/2020/10/Screenshot-2020-10-05-at-6.24.33-PM.png 1440w" sizes="(min-width: 720px) 720px"></figure><h3 id="about">About</h3><p>In this episode, I talk with <strong><a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a></strong>, who is a <strong>Principal Research Scientist and Research Lead</strong> at <strong>NVIDIA</strong>. Her research interests are in computer vision and machine learning for human-computer interaction and smart interfaces.</p><p>At NVIDIA, she has developed technologies for gaze estimation, 2D and 3D head</p>]]></description><link>http://localhost:2368/dnn-9-computer-vision-machine-learning-inside-the-car/</link><guid isPermaLink="false">5f0805899305f6020934745b</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Machine Learning]]></category><category><![CDATA[Human Computer Interaction]]></category><category><![CDATA[Podcast]]></category><dc:creator><![CDATA[Deep Neural Notebooks]]></dc:creator><pubDate>Fri, 10 Jul 2020 06:09:29 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/Podcast-thumbnails-2.009.jpeg" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-05-at-6.24.33-PM.png" class="kg-image" alt="DNN 9: NVIDIA's AI Co-Pilot , Computer Vision & ML Inside The Car // Shalini De Mello" srcset="http://localhost:2368/content/images/size/w600/2020/10/Screenshot-2020-10-05-at-6.24.33-PM.png 600w, http://localhost:2368/content/images/size/w1000/2020/10/Screenshot-2020-10-05-at-6.24.33-PM.png 1000w, http://localhost:2368/content/images/2020/10/Screenshot-2020-10-05-at-6.24.33-PM.png 1440w" sizes="(min-width: 720px) 720px"></figure><h3 id="about">About</h3><img src="http://localhost:2368/content/images/2020/10/Podcast-thumbnails-2.009.jpeg" alt="DNN 9: NVIDIA's AI Co-Pilot , Computer Vision & ML Inside The Car // Shalini De Mello"><p>In this episode, I talk with <strong><a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a></strong>, who is a <strong>Principal Research Scientist and Research Lead</strong> at <strong>NVIDIA</strong>. Her research interests are in computer vision and machine learning for human-computer interaction and smart interfaces.</p><p>At NVIDIA, she has developed technologies for gaze estimation, 2D and 3D head pose estimation, hand gesture recognition, face detection, video stabilization and GPU-optimized libraries for mobile computer vision. Her research has been focused on human-computer interaction in cars and has led to the development of NVIDIA’s innovative DriveIX product for smart AI-based automotive interfaces for future generations of cars.</p><p>Shalini received her Masters and PhD in Electrical and Computer Engineering from the University of Texas at Austin. She received a Bachelor of Engineering degree in Electronics and Electrical Communication Engineering from Punjab Engineering College.</p><h3 id="we-talk-about">We talk about</h3><p>In this episode, we talk about her journey - about how she got started with Computer Vision and Machine Learning, from her Bachelor's to her Master's in Biomedical Imaging to her PhD work on Human Face Recognition - about how her research interests shaped over the years. We also talk about Machine Learning inside the car, about her vision of using Machine Learning &amp; Deep Learning for building smart assistive interfaces for inside the car, and about how that manifested into the DriveIX product that NVIDIA recently launched. Among other things, we talk about the importance of open-sourcing technology, about the future of autonomous and semi-autonomous vehicles, about the joys of learning something new everyday, about how to keep track of the every growing amount of research and much more.</p><p>It was an absolute pleasure to talk with Shalini and learn from her research insights. I hope you like the conversation.</p><figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/Hfz965mLuvM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>Links: <a href="https://youtu.be/Hfz965mLuvM">Youtube</a> / <a href="https://tinyurl.com/ya98vk7b">Anchor.fm</a> / <a href="https://tinyurl.com/y9hu7lzq">Apple Podcasts</a> / <a href="https://tinyurl.com/yb6sn2rv">Spotify</a></p>]]></content:encoded></item><item><title><![CDATA[DNN 8: Computer Vision & ML Research, Super SloMo // Varun Jampani]]></title><description><![CDATA[<figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-05-at-6.26.55-PM.png" class="kg-image" alt srcset="http://localhost:2368/content/images/size/w600/2020/10/Screenshot-2020-10-05-at-6.26.55-PM.png 600w, http://localhost:2368/content/images/size/w1000/2020/10/Screenshot-2020-10-05-at-6.26.55-PM.png 1000w, http://localhost:2368/content/images/2020/10/Screenshot-2020-10-05-at-6.26.55-PM.png 1440w" sizes="(min-width: 720px) 720px"></figure><h3 id="about">About</h3><p>In this episode, I interview <a href="http://localhost:2368/dnn-8/varunjampani.github.io"><strong>Varun Jampani</strong></a>, who is a Research Scientist at <strong>Google Research</strong>. You might recognise him from the renowned Super SloMo paper. His work lies at the intersection of Computer Vision and Machine Learning. His main focus is to leverage machine learning techniques for better inference</p>]]></description><link>http://localhost:2368/dnn-8/</link><guid isPermaLink="false">5efdd15afc2ea1b167fe821c</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><category><![CDATA[Machine Learning]]></category><category><![CDATA[Podcast]]></category><dc:creator><![CDATA[Deep Neural Notebooks]]></dc:creator><pubDate>Tue, 02 Jun 2020 12:22:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/Podcast-thumbnails-2.008.jpeg" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/10/Screenshot-2020-10-05-at-6.26.55-PM.png" class="kg-image" alt="DNN 8: Computer Vision & ML Research, Super SloMo // Varun Jampani" srcset="http://localhost:2368/content/images/size/w600/2020/10/Screenshot-2020-10-05-at-6.26.55-PM.png 600w, http://localhost:2368/content/images/size/w1000/2020/10/Screenshot-2020-10-05-at-6.26.55-PM.png 1000w, http://localhost:2368/content/images/2020/10/Screenshot-2020-10-05-at-6.26.55-PM.png 1440w" sizes="(min-width: 720px) 720px"></figure><h3 id="about">About</h3><img src="http://localhost:2368/content/images/2020/10/Podcast-thumbnails-2.008.jpeg" alt="DNN 8: Computer Vision & ML Research, Super SloMo // Varun Jampani"><p>In this episode, I interview <a href="http://localhost:2368/dnn-8/varunjampani.github.io"><strong>Varun Jampani</strong></a>, who is a Research Scientist at <strong>Google Research</strong>. You might recognise him from the renowned Super SloMo paper. His work lies at the intersection of Computer Vision and Machine Learning. His main focus is to leverage machine learning techniques for better inference in computer vision models.</p><p>Prior to joining Google, he was a research scientist at NVIDIA.He completed his PhD at the Max-Planck Institute (MPI) for Intelligent Systems. He is also a IIIT Hyderabad alum, where he did his Bachelor's and Master's.</p><h3 id="we-talk-about">We talk about</h3><p>In this episode we talk about his journey — from his Bachelor's and Masters at IIIT Hyderabad to his PhD at MPI, about how his research has shaped over the years, about his focus on always asking good research questions and tackling fundamental problems in Computer Vision as a whole. We also talk about the SuperSloMo paper, about how it started, the key design decisions that were taken and the challenges faced in the process. If there's one thing that you are likely to take away from this conversation, it is the importance of asking good research questions and letting that drive your learning and research.</p><figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/05fGL5jEH5I?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>Links: <a href="https://youtu.be/05fGL5jEH5I">Youtube</a> / <a href="https://tinyurl.com/ycudgfse">Anchor.fm</a> / <a href="https://tinyurl.com/y8w7tkhf">Apple Podcasts</a> / <a href="https://tinyurl.com/y7u98d7m">Spotify</a></p>]]></content:encoded></item><item><title><![CDATA[DNN 1: HDR Imaging & Deep Learning for Computer Vision // Shanmuga Raman]]></title><description><![CDATA[In the first episode of the Deep Neural Notebooks podcast, I talk with Dr. Shanmuganathan Raman, who is an Associate Professor of Electrical & Computer Science Engineering at IIT Gandhinagar. He has been my mentor for my research internship at IIT.]]></description><link>http://localhost:2368/dnn-1-hdr-imaging-and-dl-computer-vision-shanmuga-raman/</link><guid isPermaLink="false">5efdcfe7fc2ea1b167fe81f2</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><category><![CDATA[Podcast]]></category><dc:creator><![CDATA[Deep Neural Notebooks]]></dc:creator><pubDate>Mon, 25 Nov 2019 12:16:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/Podcast-thumbnails-2.001.jpeg" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/10/Podcast-thumbnails-3.001-1.jpeg" class="kg-image" alt="DNN 1: HDR Imaging & Deep Learning for Computer Vision // Shanmuga Raman" srcset="http://localhost:2368/content/images/size/w600/2020/10/Podcast-thumbnails-3.001-1.jpeg 600w, http://localhost:2368/content/images/size/w1000/2020/10/Podcast-thumbnails-3.001-1.jpeg 1000w, http://localhost:2368/content/images/size/w1600/2020/10/Podcast-thumbnails-3.001-1.jpeg 1600w, http://localhost:2368/content/images/2020/10/Podcast-thumbnails-3.001-1.jpeg 1943w" sizes="(min-width: 720px) 720px"></figure><h3 id="about">About</h3><img src="http://localhost:2368/content/images/2020/10/Podcast-thumbnails-2.001.jpeg" alt="DNN 1: HDR Imaging & Deep Learning for Computer Vision // Shanmuga Raman"><p>In the first episode of the Deep Neural Notebooks podcast, I talk with <strong>Prof.</strong> <strong>Shanmuganathan Raman</strong>, who is an Associate Professor of Electrical &amp; Computer Science Engineering at <strong>IIT Gandhinagar</strong>. He has been my mentor for my research internship at IIT. He has obtained his MTech and PhD degrees from IIT Bombay and was a post-doc research associate at Department of Electrical Engineering, Indian Institute of Science (IISc), Bangalore. His PhD thesis on ‘Low Dynamic Range solution to High Dynamic Range Imaging problem’ received the IIT Bombay Excellence in PhD Thesis Work’ Award. His research interests include Computer Vision, Computational Photography, Machine Learning and Computer Graphics.</p><h3 id="we-talk-about">We talk about</h3><p>In this episode we discuss his educational background, his theses, the state of HDR imaging and computational photography. We also talk about deep learning, the scope of traditional algorithms in a DL - minded society and the road ahead.</p><figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/YJ6TWmUkhEQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><!--kg-card-begin: markdown--><p>Links: <a href="https://youtu.be/YJ6TWmUkhEQ">Youtube</a> / <a href="https://anchor.fm/deep-neural-notebooks/episodes/DNN-1-Deep-Learning-for-Computer-Vision--Computational-Photography--HDR-imaging--Shanmuganathan-Raman-e8vjlq">Anchor.fm</a> / <a href="https://podcasts.apple.com/in/podcast/deep-neural-notebooks/id1488705711?i=1000457404850">Apple Podcasts</a> / <a href="https://open.spotify.com/episode/6d2EFs6Lke0gW4UMJj5jpO">Spotify</a></p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑]]></title><description><![CDATA[<p>CVPR 2017, Best Paper Award winner</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt><figcaption>Dense connections</figcaption></figure><blockquote><strong>“Simple models and a lot of data trump more elaborate models based on less data. “ — </strong>Peter Norvig</blockquote><h3 id="about-the-paper">About the paper</h3><p>‘<strong>Densely Connected Convolutional Networks</strong>’ received the <strong>Best Paper Award</strong> at the IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>) <strong>2017</strong>. The</p>]]></description><link>http://localhost:2368/densenet-densely-connected-convolutional-networks/</link><guid isPermaLink="false">5f93aa97f123610eb9e42a54</guid><category><![CDATA[8k+ views]]></category><category><![CDATA[Paper Review]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Sun, 10 Nov 2019 04:23:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/10/1_TeHVqikNc68QC98Vm9M98Q.gif" medium="image"/><content:encoded><![CDATA[<img src="http://localhost:2368/content/images/2020/10/1_TeHVqikNc68QC98Vm9M98Q.gif" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><p>CVPR 2017, Best Paper Award winner</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Dense connections</figcaption></figure><blockquote><strong>“Simple models and a lot of data trump more elaborate models based on less data. “ — </strong>Peter Norvig</blockquote><h3 id="about-the-paper">About the paper</h3><p>‘<strong>Densely Connected Convolutional Networks</strong>’ received the <strong>Best Paper Award</strong> at the IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>) <strong>2017</strong>. The paper can be read <a href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener">here</a>.</p><p>The primary author, <a href="http://www.gaohuang.net/" rel="noopener"><strong>Gao Huang</strong></a> has been a Postdoctoral Fellow at Cornell University and is currently working at Tsinghua University as an Assistant Professor. His research focuses on deep learning for computer vision.</p><h3 id="how-i-came-across-the-paper">How I came across the paper?</h3><p>I came across this paper while researching for neural network implementations that were focused on improving image quality (in terms of resolution or dynamic range) by reconstruction. Although this paper demonstrates the prowess of the architecture in image classification, the idea of dense connections has inspired optimisations in many other deep learning domains like image super-resolution, image segmentation, medical diagnosis etc.</p><h3 id="key-contributions-of-the-densenet-architecture">Key contributions of the DenseNet architecture</h3><ul><li>Alleviates <strong>vanishing gradient problem</strong></li><li>Stronger <strong>feature propagation</strong></li><li><strong>Feature reuse</strong></li><li><strong>Reduced parameter</strong> count</li></ul><blockquote><strong>Before you read: </strong><br>Understanding this post requires a basic understanding of deep learning concepts.</blockquote><h3 id="paper-review">Paper review</h3><p>The paper starts with talking about the <strong>vanishing gradient problem — </strong>about how, as networks get deeper, gradients aren’t back-propagated sufficiently to the initial layers of the network. The gradients keep getting smaller as they move backwards into the network and as a result, the initial layers lose their capacity to learn the basic low-level features.</p><p>Several architectures have been developed to solve this problem. These include — ResNets, Highway Networks, Fractal Nets, Stochastic depth networks.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*MwxLN83uT8bgd4nGiR6JZQ.jpeg" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"></figure><p>Regardless of the architectural designs of these networks, they all try to create channels for information to flow between the initial layers and the final layers. DenseNets, with the same objective, create paths between the layers of the network.</p><h3 id="related-works">Related works</h3><ul><li>Highway networks (one of the first attempts at making training easy for deeper models)</li><li>ResNet (Bypassing connections by summation using identity mappings)</li><li>Stochastic depth (dropping layers randomly during training)</li><li>GoogLeNet (inception module — increasing network width)</li><li>FractalNet</li><li>Network in Network (NIN)</li><li>Deeply Supervised Network (DSN)</li><li>Ladder Networks</li><li>Deeply-Fused Nets (DFNs)</li></ul><h3 id="dense-connections">Dense connections</h3><p>Following the feed-forward nature of the network, each layer in a dense block receives feature maps from all the preceding layers, and passes its output to all subsequent layers. Feature maps received from other layers are fused through <strong>concatenation</strong>, and not through summation (like in ResNets).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*StSq7XyHcuxUOfWuuALkdw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Concatenation of feature&nbsp;maps</figcaption></figure><p>These connections form a dense circuit of pathways that allow <strong>better gradient-flow</strong>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*TeHVqikNc68QC98Vm9M98Q.gif" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Dense connections</figcaption></figure><blockquote>Each layer has direct access to the gradients of the loss function and the original input signal.</blockquote><p>Because of these dense connections, the model requires fewer layers, as there is no need to learn redundant feature maps, allowing the <strong>collective knowledge </strong>(features learnt collectively by the network) to be reused. The proposed architecture has narrow layers, which provide state-of-the-art results for as low as 12 channel feature maps. Fewer and narrower layers means that the model has <strong>fewer parameters</strong> to learn, making them easier to train. The authors also talk about the importance of variation in input of layers as a result of concatenated feature maps, which prevents the model from over-fitting the training data.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*04TJTANujOsauo3foe0zbw.jpeg" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"></figure><p>Many variants of the DenseNet model have been presented in the paper. I have opted to explain the concepts with their standard network (DenseNet-121).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*rkra9kVPl754-vjRGsOqvw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Some of the variants of the DenseNet architecture</figcaption></figure><h3 id="composite-function">Composite function</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*qA2rLVBRB-wZoI3nAEMdkA.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Composite function</figcaption></figure><p>*Each CONV block in the network representations in the paper (and in the blog) corresponds to an operation of —</p><p><strong>BatchNorm→ReLU→Conv*</strong></p><h3 id="dense-block">Dense block</h3><p>The concept of dense connections has been portrayed in dense blocks. A dense block comprises <em>n </em>dense layers. These dense layers are connected using a dense circuitry such that each dense layer receives feature maps from all preceding layers and passes it’s feature maps to all subsequent layers. The dimensions of the features (width, height) stay the same in a dense block.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*ZfrliiHwn_L4kKcO61Oxgw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><strong class="markup--strong markup--figure-strong">Dense block (DB) with six Dense Layers&nbsp;(DL)</strong></figcaption></figure><h3 id="dense-layer">Dense layer</h3><p>Each dense-layer consists of 2 convolutional operations -</p><ul><li><strong>1 X 1 CONV </strong>(conventional conv operation for extracting features)</li><li><strong>3 X 3 CONV </strong>(bringing down the feature depth/channel count)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*cRXqccOxYkZbWpfmyXzjog.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><strong class="markup--strong markup--figure-strong">Dense layer of&nbsp;DB-1</strong></figcaption></figure><p>The DenseNet-121 comprises of 6 such dense layers in a dense block. The depth of the output of each dense-layer is equal to the growth rate of the dense block.</p><h3 id="growth-rate-k-">Growth rate (k)</h3><p>This is a term you’ll come across a lot in the paper. It is basically the number of channels output by a dense-layer (<em>1x1 conv → 3x3 conv</em>). The authors have used a value of <em>k = 32</em> for the experiments. This means that the number of features received by a dense layer ( <em>l </em>) from it’s preceding dense layer ( <em>l-1</em> ) is 32. This is referred to as the growth rate because after each layer, 32 channel features are concatenated and fed as input to the next layer.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*NIQenf9KTillNhYfuySfMw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Dense block with channel count (C) of features entering and exiting the&nbsp;layers</figcaption></figure><h3 id="transition-layer">Transition layer</h3><p>At the end of each dense block, the number of feature-maps accumulates to a value of — <em>input features + (number of dense layers x growth rate). </em>So for 64 channel features entering a dense block of 6 dense-layers of growth rate 32, the number of channels accumulated at the end of the block will be — <br>64 + (6 x 32) = 256. To bring down this channel count, a <strong>transition layer </strong>(or block) is added between two dense blocks. The transition layer consists of -</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*LoFEV57u5kCjsqZRX7EAKw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><strong class="markup--strong markup--figure-strong">Transition layer/block</strong></figcaption></figure><ul><li><strong>1 X 1 CONV</strong> operation</li><li><strong>2 X 2 AVG POOL </strong>operation</li></ul><p>The <strong>1 X 1 CONV </strong>operation reduces the channel count to half.<br> The <strong>2 X 2 AVG POOL</strong> layer is responsible for downsampling the features in terms of the width and height.</p><h3 id="full-network">Full network</h3><p>As can be seen in the diagram below, the authors have chosen different number of dense layers for each of the three dense block.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*CE11_lfEz00aoOjLiw5sdw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><strong class="markup--strong markup--figure-strong">Full DenseNet architecture</strong></figcaption></figure><h3 id="comparison-with-densenet">Comparison with DenseNet</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/600/1*cXAoIC_ig5R8nE4hKj2OeQ.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>ResNet DenseNet comparison</figcaption></figure><p>We can see that even with a reduced parameter count, the DenseNet model has a significantly lower validation error for the ResNet model with the same number of parameters. These experiments were carried out on both the models with hyper-parameters that suited ResNet better. The authors claim that DenseNet would perform better after extensive hyper-parameter searches.</p><blockquote>DenseNet-201 with 20M parameters model yields similar validation error as a 101-layer ResNet with more than 40M parameters.</blockquote><h3 id="inspecting-the-code">Inspecting the code</h3><p>I believe that going through the code makes it easier to understand the implementations of such architectures. Research papers (in the context of Deep Learning) can be difficult to understand because they are more about what drives the design decisions of a neural network. Inspecting the code (usually the network/model code) can reduce this complexity because sometimes it’s just the implementation that we are interested in. Some people prefer first seeing the implementation and then trying to figure out the reasoning behind the design decisions of the network. Regardless, reading the code, before or after, always helps.</p><p>The code of the DenseNet implementation can be found <a href="https://github.com/liuzhuang13/DenseNet" rel="noopener">here</a>. Since I am more comfortable with PyTorch, I’ll try to explain the PyTorch implementation of the model which can be found <a href="https://github.com/gpleiss/efficient_densenet_pytorch" rel="noopener">here</a>. The most important file would be <strong>models/densenet.py</strong>, that hold the network architecture for DenseNet.</p><p>The code has been divided into these classes where each type of block is represented by a class.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*RgsliXFElnLQ5uh5ZCDBAg.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption>Class hierarchy in&nbsp;code</figcaption></figure><h3 id="dense-layer-1">Dense layer</h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/1200979cc91e54f2e638872fd4680560.js"></script>
<!--kg-card-end: html--><p>The<strong> _DenseLayer </strong>class can be used to initialise the constituent operations of a dense layer —</p><p><strong>BatchNorm → ReLU → Conv (1X1) → BatchNom → ReLU → Conv (3X3)</strong></p><p>The <strong>_bn_function_factory()</strong> function is responsible for concatenating the output of the previous layers to the current layer.</p><h3 id="denseblock"><strong>DenseBlock</strong></h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/0783bc22b0b8f826ef92e1e7455c3075.js"></script><!--kg-card-end: html--><p>The _<strong>DenseBlock </strong>class houses a certain number of <strong>_DenseLayer</strong>s (<em>num_layers</em>).<br>This class is initialised from the <strong>DenseNet </strong>class depending on the number of dense blocks used in the network.</p><h3 id="transition-block">Transition Block</h3><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/86feacfbdbbc4ad841adac0e00e4de75.js"></script><!--kg-card-end: html--><h3 id="densenet"><strong>DenseNet</strong></h3><p>Since this part of the code is a little too big to fit in this blog, I’ll just be using a part of the code that should help in getting the gist of the network.</p><!--kg-card-begin: html--><script src="https://gist.github.com/mukulkhanna/2238b561aca1c29878310fe322f1ba54.js"></script><!--kg-card-end: html--><p>I found this image online and it has helped me understand the network better.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*vIZhPImFr9Gjpx6ZB7IOJg.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"><figcaption><a href="https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a" data-href="https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Source</a></figcaption></figure><h3 id="other-works-inspired-from-this-paper">Other works inspired from this paper</h3><ul><li><a href="https://arxiv.org/abs/1802.08797" rel="noopener">Residual Dense Network for Image Super-Resolution</a> (2018)</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*6NS5NPZoU3iQXIJOu6jruQ.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"></figure><ul><li><a href="http://www.statnlp.org/research/lg/zhijiang_zhangyan19tacl-gcn.pdf" rel="noopener">Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning</a> (2019)</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*pa45Gjp8H1D9NnXJE6Vypw.png" class="kg-image" alt="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑"></figure><h3 id="conclusion">Conclusion</h3><p>DenseNet is a network that portrays the importance of having dense connections in a network using dense blocks. This helps in feature-reuse, better gradient flow, reduced parameter count and better transmission of features across the network. Such an implementation can help in training deeper networks using less computational resources and with better results.<br></p>]]></content:encoded></item><item><title><![CDATA[HDR Imaging: What is an HDR image anyway? 📷]]></title><description><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/0*E1pzpUrIPoMfaWNg.jpg" class="kg-image" alt></figure><p>We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn’t turn out well. The image comes out to be either too dark or too bright depending on the focus. Let’s try to understand why this happens and how this</p>]]></description><link>http://localhost:2368/what-is-an-hdr-image-anyway/</link><guid isPermaLink="false">5f93a47ef123610eb9e42a04</guid><category><![CDATA[4k+ views]]></category><category><![CDATA[HDR]]></category><category><![CDATA[Computer Vision]]></category><dc:creator><![CDATA[Mukul Khanna]]></dc:creator><pubDate>Thu, 10 Oct 2019 03:56:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/0*E1pzpUrIPoMfaWNg.jpg" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"></figure><img src="https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="HDR Imaging: What is an HDR image anyway? 📷"><p>We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn’t turn out well. The image comes out to be either too dark or too bright depending on the focus. Let’s try to understand why this happens and how this can be solved.</p><p>There are a lot of key concepts that revolve around the study of HDR images -</p><ul><li><strong>Dynamic range</strong></li><li><strong>Image exposure</strong></li><li><strong>Shutter speed, Aperture, ISO</strong></li><li><strong>Image bracketing</strong></li><li><strong>Merging LDR images</strong></li><li><strong>Image encoding</strong></li><li><strong>Camera response function</strong></li><li><strong>Linearisation</strong></li><li><strong>Gamma correction</strong></li><li><strong>Tonemapping</strong></li><li><strong>Visualising HDR images</strong></li></ul><h3 id="dynamic-range">Dynamic Range</h3><p>Dynamic range of a scene refers to the range of light intensity that encompasses a scene. It can also be defined as the ratio of light (maximum measurable brightness) to dark (minimum measurable brightness) in an image.</p><p>To get some context of the how brightness can be quantised, the range of light intensity is 0 to infinity, with zero being the darkest and infinity being the brightest source there is (☀️) .</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*YX1gBta3mldqWhvBMUmqsA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"><figcaption>Luminance value comparison (<a href="https://www.hdrsoft.com/resources/dri.html" rel="noopener">Source</a>)</figcaption></figure><p>No camera is able to capture this complete uncapped range of illuminance in a scene. Therefore, images turn out to be either too bright (overexposed) or too dark (underexposed). These images are called <em>Low Dynamic Range</em> (LDR) images . For images that turn out too bright, it is only the brighter subrange (of the infinite range) that the camera is able to capture, and correspondingly for the darker images, only the lower subrange is captured.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*CzMITGf7VZ6ly2wLKz_d1g.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"><figcaption><strong>True image</strong> (Left), <strong>Overexposed image</strong> (Center), <strong>Underexposed image</strong> (Right)</figcaption></figure><h3 id="image-exposure">Image Exposure</h3><p>The amount of light entering the camera (and thus, the image) is called the exposure. The exposure of the image can be controlled by three settings of a camera — the aperture, shutter speed and ISO.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*mQ_fmmAzRkmKX63SrfCV8A.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"></figure><p><strong>Aperture:</strong> The area of the camera lens through which the light can come in.</p><p><strong>Shutter speed:</strong> The speed with which the shutter of the camera closes. As the shutter speed increases, the amount of light entering the camera decreases, and vice versa. It also improves the sharpness of an image.</p><p><strong>ISO: </strong>Sensitivity of the camera sensor to incoming light.</p><p>I found this nice analogy <a href="https://www.cambridgeincolour.com/tutorials/camera-exposure.htm" rel="noopener">here</a>, between the camera settings and a bucket left out in the rain.</p><blockquote>In photography, the exposure settings of aperture, shutter speed and ISO speed are analogous to the width, time and quantity in the discussion above. Furthermore, just as the rate of rainfall was beyond your control above, so too is natural light for a photographer.</blockquote><p>Back to <em>dynamic range</em>. A single image captured from the camera can not contain the wide range of light intensity.</p><p>This problem can be solved by merging images captured at multiple exposure values. How this helps is that the overexposed images work well for the darker regions in the image, and the underexposed images are able to tone down the intensity in the extra-bright regions. Different regions of the image are captured better at different exposure values. Therefore, the idea is to merge these set of images and to recover an image with a <em>high dynamic range</em> (HDR).</p><h3 id="image-bracketing">Image bracketing</h3><p>Bracketing refers to capturing multiple images of the same scene with different camera settings. It is usually done automatically by the camera. What happens when you use the HDR feature on your smart phone is that the phone captures 3 (usually) images at three different exposure times (or exposure values) in quick succession. The lower the exposure time, the lesser the amount of light that gets in. These three images are merged by the camera software and are saved as a single image, in a way that the best portions of each image make it to the final image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*gkqV3yHAhfQXEYGY.jpg" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"><figcaption>A collage of 5 bracketed images that I found on the <a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiR0PPA4JHjAhVQb30KHdH6DaUQjhx6BAgBEAM&amp;url=https%3A%2F%2Fwww.highdynamicranger.com%2Fhow-to-use-auto-exposure-bracketing-for-hdr%2F&amp;psig=AOvVaw0b8jjGtlbQ010BQW1YHPIQ&amp;ust=1562002928887447" rel="noopener">internet</a>.</figcaption></figure><p>The funny thing here is that the image that is saved on your phone after the merging happens, is still not (technically) an HDR image. This is where the image encodings come into the picture (and also tonemapping — which we’ll discuss later).</p><h3 id="image-encoding">Image encoding</h3><p>Commonly, the images that we see on our phones and computers, are 8-bit (per channel) encoded RGB images. Each pixel’s value is stored using 24-bit representations, 8-bit for each channel (R, G, B). Each channel of a pixel has a range of 0–255 intensity values.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*KyT2vpk6-OCt_Nu9SXCNyw.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"><figcaption>Example of 24-bit (3x8-bit) encoding for an RGB pixel</figcaption></figure><p>The problem with this encoding that it is not capable of containing the large dynamic range of natural scenes. It only allows a range of 0–255 (only integers) for accommodating the intensity range, which is not sufficient.</p><p>To solve this problem, HDR images are encoded using 32-bit floating point numbers, for each channel. This allows us to capture the wide uncapped range of HDR images. There are various formats for writing HDR images, the most common being .hdr and .exr. All HDR images are 32-bit encodings but not all 32-bit images can be HDR images.</p><h3 id="camera-response-function-crf-">Camera Response Function (CRF)</h3><p>CRF is a function that shows the relationship between the actual scene irradiance and the digital brightness values in the image. It is also called as the Opto-electrical transfer function. Camera companies don’t provide their CRFs and consider it as proprietary information.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*XA43PkIl_VE44Asilb5z2w.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"><figcaption>Camera response function example</figcaption></figure><p>In an ideal world, the CRF should have a linear graph — meaning, the brightness value of the pixels in the image should be directly proportional to the actual irradiance in the scene. This is true for HDR images, but not for the usual images where the brightness values are altered to be able to contain them in a finite range. The more important reason for conventional images being de-linearised depends on how display devices work.</p><p>Back in the time of CRT (Cathode Ray Tube) displays, electrons were fired on a phosphor surface. The phosphor screen is known to emit photons upon being hit by accelerated electrons. However the brightness of the display didn’t vary linearly with the strength of the electron beam. This problem has been solved by modifying the incoming image/video source signals non-linearly in the direction opposite to the display’s non-linearity. By doing this, we can get a fair linear estimate of the natural scene brightness.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*UW_uGegQxkl17PlHU0f2WA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"></figure><p>This de-linearisation of the source allows to compensate for a non-linear display. Display technologies have advanced but the non-linearity still exists in most devices. This <em>de-linearisation</em> is known as gamma correction.</p><blockquote>Gamma corrected image = image ^ γ</blockquote><p>If the input image is x, then what a display device of gamma=1.2 shows is x^(1.2). Therefore, the input image is encoded as x^(1/1.2) so that the monitor converts it to x^((1/1.2) x 1.2) which is equal to x, the original image captured by the camera.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*4k6-goBZ-RVLWjd9psEXAA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"></figure><p>For most displays these days, images have to be encoded by a gamma value of 0.45 (1/2.2) because of a gamma decoding of 2.2 by the displays.</p><p>Gamma encoding is performed over a range of [0,1]. So images first have to be normalised by dividing by 255 and then again multiplying by 255 after the gamma operation. Powers of greater than 1 yield darker images whereas powers of less than 1 yield brighter image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*grxUUm_ZH2CmaLs-ynH0EA.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"><figcaption>Comparison between gamma encoded images</figcaption></figure><p>HDR photography (or any photography) is quite complex in a way that we need to think of three important aspects —</p><ul><li>How the actual scene is (ground truth / uncapped dynamic range)</li><li>How the camera captures (bracketing and then merging)</li><li>How it is displayed (tonemapping)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*wNoLL8Iz5IIR-pEb.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"><figcaption><a href="https://www.cambridgeincolour.com/tutorials/dynamic-range.htm" data-href="https://www.cambridgeincolour.com/tutorials/dynamic-range.htm" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Source</a></figcaption></figure><p>In the context of HDR imaging, we have discussed the first two points. Now let’s look at how HDR images can be displayed.</p><h3 id="tonemapping">Tonemapping</h3><p>Most off the shelf display devices are incapable of delivering the wide uncapped range of HDR images. They expect the input source to be in the three-channel 24-bit (3x8) RGB format. Due to this reason, the wide dynamic range needs to be toned down to be able to accommodate it in the 0–255 range of RGB format. This can be done in several ways, some of which are-</p><ul><li>Conventional linear normalisation: This is the most basic way of bringing down the wide range of an HDR image.</li></ul><blockquote>tonemapped image = (img — img.min()/img.max() — img.min()) x 255</blockquote><ul><li>Reinhard tonemapping: This is one of the most commonly used tonemapping algorithm that was shared in this <a href="http://www.cmap.polytechnique.fr/~peyre/cours/x2005signal/hdr_photographic.pdf" rel="noopener">paper</a>.</li></ul><blockquote>tonemapped image = img/(1+img) x 255</blockquote><ul><li>Drago tonemapping: This tonemapper is a perception-based one that compresses the dynamic range using logarithmic functions “computed using different bases from the scene content”. The paper for this can be found <a href="http://resources.mpi-inf.mpg.de/tmo/logmap/logmap.pdf" rel="noopener">here</a>.</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*0gbgFTFg_mlZRqdWa7b8qw.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"></figure><p>You asked for it</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*9Ss_5MkHidaiJcIU67vHUg.png" class="kg-image" alt="HDR Imaging: What is an HDR image anyway? 📷"><figcaption>Results</figcaption></figure><h3 id="ongoing-research-in-generating-hdr-content">Ongoing research in generating HDR content</h3><p>The conventional approach of generating HDR content is by merging multiple images captured at different exposures (bracketing). However, this approach is likely to create ghosting (blur) artifacts when there is movement between the frames. This has been solved by first aligning the neighbouring frame with the reference frame (middle frame) using something known as <em>optical flow</em>. That can be a topic for another blog post but for now we can think of it as a way to estimate the motion of objects (or pixels) that happens across frames by assigning a displacement vector to certain pixel positions.</p><p>There also has been work in generating HDR frames from singular LDR counterparts using Deep Learning. Neural networks are successfuly able to learn complex representations between the input and the output, and have thus performed quite well in learning the LDR to HDR mapping. These are some of the state-of-the-art methods for HDR image generation from a single image —</p><ul><li>HDRCNN: <a href="http://hdrv.org/hdrcnn/material/sga17_paper.pdf" rel="noopener">HDR image reconstruction from a single exposure using deep CNNs</a></li><li><a href="https://arxiv.org/pdf/1903.01277.pdf" rel="noopener">Deep Inverse Tone Mapping Using LDR Based Learning for Estimating HDR Images with Absolute Luminance</a></li><li><a href="https://arxiv.org/abs/1803.02266" rel="noopener">ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content</a></li></ul><p>Here are some state-of-the-art Deep Learning based methods for HDR image generation using multiple LDR images —</p><ul><li><a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Deep High Dynamic Range Imaging of Dynamic Scenes</a></li><li>AHDRNet: <a href="https://arxiv.org/abs/1904.10293" rel="noopener">Attention-guided Network for Ghost-free High Dynamic Range Imaging</a></li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://towardsdatascience.com/paper-review-attention-guided-network-for-ghost-free-high-dynamic-range-imaging-4df2ec378e8"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Paper review — Attention-guided Network for Ghost-free High Dynamic Range Imaging</div><div class="kg-bookmark-description">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at CVPR 2019…</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png" alt="HDR Imaging: What is an HDR image anyway? 📷"><span class="kg-bookmark-author">Mukul Khanna</span><span class="kg-bookmark-publisher">Towards Data Science</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://miro.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg" alt="HDR Imaging: What is an HDR image anyway? 📷"></div></a></figure><h3 id="how-to-view-hdr-images">How to view HDR images</h3><p>HDR images are stored as luminance maps and not as conventional RGB images and thus can’t be viewed using common image viewing applications.</p><p>MacOS allows you to view .hdr and .exr files using the Preview and Finder app. You can also use the <a href="https://viewer.openhdr.org/" rel="noopener">OpenHDR</a> website to visualise such images.</p><hr><p>Thanks for reading.</p><p>For future blog posts, I would like to discuss about how operations can be performed on HDR images using Python, OpenCV and Numpy. I would also like to share the current scenario of research being done for HDR video generation.</p>]]></content:encoded></item></channel></rss>