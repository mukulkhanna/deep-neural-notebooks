<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>HDR Imaging: What is an HDR image anyway? 📷</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="preload" href="../assets/css/app.css?v=56642d80f4" as="style">
    <link rel="preload" href="../assets/js/manifest.js?v=56642d80f4" as="script">
    <link rel="preload" href="../assets/js/vendor/content-api.min.js?v=56642d80f4" as="script">
    <link rel="preload" href="../assets/js/vendor.js?v=56642d80f4" as="script">
    <link rel="preload" href="../assets/js/app.js?v=56642d80f4" as="script">
    <link rel="preconnect" href="https://polyfill.io">
    <link rel="dns-prefetch" href="https://polyfill.io">

      <link rel="preload" href="../assets/css/post.css?v=56642d80f4" as="style">
  <link rel="preload" href="../assets/js/post.js?v=56642d80f4" as="script">


    <style>
      /* These font-faces are here to make fonts work if the Ghost instance is installed in a subdirectory */

      /* source-sans-pro-regular */
      @font-face {
        font-family: 'Source Sans Pro';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: local('Source Sans Pro Regular'), local('SourceSansPro-Regular'),
            url("../assets/fonts/source-sans-pro/source-sans-pro-regular.woff2?v=56642d80f4") format('woff2'),
            url("../assets/fonts/source-sans-pro/source-sans-pro-regular.woff?v=56642d80f4") format('woff');
      }

      /* source-sans-pro-600 */
      @font-face {
        font-family: 'Source Sans Pro';
        font-style: normal;
        font-weight: 600;
        font-display: swap;
        src: local('Source Sans Pro SemiBold'), local('SourceSansPro-SemiBold'),
            url("../assets/fonts/source-sans-pro/source-sans-pro-600.woff2?v=56642d80f4") format('woff2'),
            url("../assets/fonts/source-sans-pro/source-sans-pro-600.woff?v=56642d80f4") format('woff');
      }

      /* source-sans-pro-700 */
      @font-face {
        font-family: 'Source Sans Pro';
        font-style: normal;
        font-weight: 700;
        font-display: swap;
        src: local('Source Sans Pro Bold'), local('SourceSansPro-Bold'),
            url("../assets/fonts/source-sans-pro/source-sans-pro-700.woff2?v=56642d80f4") format('woff2'),
            url("../assets/fonts/source-sans-pro/source-sans-pro-700.woff?v=56642d80f4") format('woff');
      }

      /* iconmoon */
      @font-face {
        font-family: 'icomoon';
        font-weight: normal;
        font-style: normal;
        font-display: swap;
        src: url("../assets/fonts/icomoon/icomoon.eot?aoz2mo?v=56642d80f4");
        src: url("../assets/fonts/icomoon/icomoon.eot?aoz2mo") format('embedded-opentype'),
        url("../assets/fonts/icomoon/icomoon.ttf?aoz2mo?v=56642d80f4") format('truetype'),
        url("../assets/fonts/icomoon/icomoon.woff?aoz2mo?v=56642d80f4") format('woff'),
        url("../assets/fonts/icomoon/icomoon.svg?aoz2mo") format('svg');
      }
    </style>

    <link rel="stylesheet" type="text/css" href="../assets/css/app.css?v=56642d80f4" media="screen">

      <link rel="stylesheet" type="text/css" href="../assets/css/post.css?v=56642d80f4" media="screen">


    

    <link rel="icon" href="../favicon.png" type="image/png">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="Deep Neural Notebooks">
    <meta property="og:type" content="article">
    <meta property="og:title" content="HDR Imaging: What is an HDR image anyway? 📷">
    <meta property="og:description" content="We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn’t turn out well. The image comes out to be either too dark or too bright depending on the focus. Let’s try to understand why this happens and how this">
    <meta property="og:url" content="http://localhost:2368/what-is-an-hdr-image-anyway/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ">
    <meta property="article:published_time" content="2019-10-10T03:56:00.000Z">
    <meta property="article:modified_time" content="2020-10-24T05:58:48.000Z">
    <meta property="article:tag" content="4k+ views">
    <meta property="article:tag" content="HDR">
    <meta property="article:tag" content="Computer Vision">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="HDR Imaging: What is an HDR image anyway? 📷">
    <meta name="twitter:description" content="We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn’t turn out well. The image comes out to be either too dark or too bright depending on the focus. Let’s try to understand why this happens and how this">
    <meta name="twitter:url" content="http://localhost:2368/what-is-an-hdr-image-anyway/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Mukul Khanna">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="4k+ views, HDR, Computer Vision">
    <meta name="twitter:site" content="@mkulkhanna">
    <meta name="twitter:creator" content="@mkulkhanna">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="3000">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Deep Neural Notebooks",
        "url": "http://localhost:2368/",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/content/images/2020/07/imageonline-co-whitebackgroundremoved-2.png",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Mukul Khanna",
        "image": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/content/images/2020/07/3CA5BB6B-761F-4F3F-8BEC-32E58969F08E-3.JPG",
            "width": 2000,
            "height": 1981
        },
        "url": "http://localhost:2368/author/mukul/",
        "sameAs": [
            "http://mukulkhanna.co",
            "https://twitter.com/mkulkhanna"
        ]
    },
    "headline": "HDR Imaging: What is an HDR image anyway? 📷",
    "url": "http://localhost:2368/what-is-an-hdr-image-anyway/",
    "datePublished": "2019-10-10T03:56:00.000Z",
    "dateModified": "2020-10-24T05:58:48.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1548681528-6a5c45b66b42?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ",
        "width": 2000,
        "height": 3000
    },
    "keywords": "4k+ views, HDR, Computer Vision",
    "description": "We all have noticed how capturing images with the sun (or any bright objects) in\nthe background usually doesn’t turn out well. The image comes out to be either\ntoo dark or too bright depending on the focus. Let’s try to understand why this\nhappens and how this can be solved.\n\nThere are a lot of key concepts that revolve around the study of HDR images -\n\n * Dynamic range\n * Image exposure\n * Shutter speed, Aperture, ISO\n * Image bracketing\n * Merging LDR images\n * Image encoding\n * Camera respons",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 4.2">
    <link rel="alternate" type="application/rss+xml" title="Deep Neural Notebooks" href="../rss/index.html">
    <script defer src="https://unpkg.com/@tryghost/portal@~1.1.0/umd/portal.min.js" data-ghost="http://localhost:2368/"></script><style> .gh-post-upgrade-cta-content,
.gh-post-upgrade-cta {
    display: flex;
    flex-direction: column;
    align-items: center;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    text-align: center;
    width: 100%;
    color: #ffffff;
    font-size: 16px;
}

.gh-post-upgrade-cta-content {
    border-radius: 8px;
    padding: 40px 4vw;
}

.gh-post-upgrade-cta h2 {
    color: #ffffff;
    font-size: 28px;
    letter-spacing: -0.2px;
    margin: 0;
    padding: 0;
}

.gh-post-upgrade-cta p {
    margin: 20px 0 0;
    padding: 0;
}

.gh-post-upgrade-cta small {
    font-size: 16px;
    letter-spacing: -0.2px;
}

.gh-post-upgrade-cta a {
    color: #ffffff;
    cursor: pointer;
    font-weight: 500;
    box-shadow: none;
    text-decoration: underline;
}

.gh-post-upgrade-cta a:hover {
    color: #ffffff;
    opacity: 0.8;
    box-shadow: none;
    text-decoration: underline;
}

.gh-post-upgrade-cta a.gh-btn {
    display: block;
    background: #ffffff;
    text-decoration: none;
    margin: 28px 0 0;
    padding: 8px 18px;
    border-radius: 4px;
    font-size: 16px;
    font-weight: 600;
}

.gh-post-upgrade-cta a.gh-btn:hover {
    opacity: 0.92;
}</style>
    <script>
  const ghostSearchApiKey = 'efff2b0d00118a51535099c413'
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1H8F4BQ5WV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1H8F4BQ5WV');
</script>

<!-- prism.js -->
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/themes/prism-tomorrow.min.css">

<style>
  pre[class*="language-"] {
    margin: 0 0 1.5em !important;
  }
  code {
    text-shadow: none !important;
  }
  .token.operator {
    background: none !important;
  }
  :not(pre) > code[class*="language-"],
  pre[class*="language-"] {
    background: #20262E !important;
  }

</style>
<style>:root {--ghost-accent-color: #de54ab;}</style>

    <script>
      // @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&dn=expat.txt Expat
      const ghostHost = "http://localhost:2368"
      // @license-end
    </script>
  </head>
  <body class="post-template tag-4k-views-2 tag-hdr tag-computer-vision">
    



  
<header class="m-header with-picture js-header">
  <div class="m-mobile-topbar" data-aos="fade-down">
    <button class="m-icon-button in-mobile-topbar js-open-menu" aria-label="Open menu">
      <span class="icon-menu" aria-hidden="true"></span>
    </button>
      <a href="../index.html" class="m-logo in-mobile-topbar">
        <img src="../content/images/2020/07/imageonline-co-whitebackgroundremoved-2.png" alt="Deep Neural Notebooks">
      </a>
    <button class="m-icon-button in-mobile-topbar js-open-search" aria-label="Open search">
      <span class="icon-search" aria-hidden="true"></span>
    </button>
  </div>

  <div class="m-menu js-menu">
    <button class="m-icon-button outlined as-close-menu js-close-menu" aria-label="Close menu">
      <span class="icon-close"></span>
    </button>
    <div class="m-menu__main" data-aos="fade-down">
      <div class="l-wrapper">
        <div class="m-nav js-main-nav">
          <nav class="m-nav__left js-main-nav-left" role="navigation" aria-label="Main menu">
            <ul>
                <li class="only-desktop">
                  <a href="../index.html" class="m-logo">
                    <img src="../content/images/2020/07/imageonline-co-whitebackgroundremoved-2.png" alt="Deep Neural Notebooks">
                  </a>
                </li>
                
    <li class="nav-mukul-khanna">
      <a href="../about/index.html">Mukul Khanna</a>
    </li>
    <li class="nav-podcast">
      <a href="../index.html">podcast</a>
    </li>
    <li class="nav-blog">
      <a href="../author/mukul/index.html">blog</a>
    </li>
    <li class="nav-youtube">
      <a href="https://www.youtube.com/channel/UC66w1T4oMv66Jn1LR5CW2yg">youtube</a>
    </li>

            </ul>
          </nav>
          <div class="m-nav__right">
            <button class="m-icon-button in-menu-main js-open-search" aria-label="Open search">
              <span class="icon-search" aria-hidden="true"></span>
            </button>
            <div class="m-toggle-darkmode js-tooltip" data-tippy-content="Toggle dark mode" tabindex="0">
              <label for="toggle-darkmode" class="sr-only">
                Toggle dark mode
              </label>
              <input id="toggle-darkmode" type="checkbox" class="js-toggle-darkmode">
              <div>
                <span class="icon-moon moon" aria-hidden="true"></span>
                <span class="icon-sunny sun" aria-hidden="true"></span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

</header>

<main class="main-wrap" style="margin-top:4%">
    
  
  <article>
    <div class="l-content in-post">
        <div class="l-wrapper in-post  js-aos-wrapper" data-aos="fade-up" data-aos-delay="300">
          <div class="l-post-content  has-subscribe-form js-progress-content">
            <header class="m-heading">
              <h1 class="m-heading__title in-post">HDR Imaging: What is an HDR image anyway? 📷</h1>
              <div class="m-heading__meta">
                  <a href="../tag/4k-views-2/index.html" class="m-heading__meta__tag">4k+ views</a>
                  <span class="m-heading__meta__divider" aria-hidden="true">•</span>
                <span class="m-heading__meta__time">Oct 10, 2019</span>
              </div>
            </header>
            <div class="pos-relative js-post-content">
              <div class="m-share">
                <div class="m-share__content js-sticky">
                  <a href="https://www.youtube.com/playlist?list=PLKsk3K4Z-1AVwIzEi9pk-ayEh9uxdyweL" class="m-icon-button filled in-share" target="_blank" rel="noopener" aria-label="Youtube">
                    <span class="icon-youtube" aria-hidden="true"></span>
                  </a>
                  <a href="https://open.spotify.com/show/2eq1jD7V5K19aZUUJnIz5z" class="m-icon-button filled in-share" target="_blank" rel="noopener" aria-label="Spotify">
                    <span class="icon-spotify" aria-hidden="true"></span>
                  </a>
                  <button class="m-icon-button filled in-share progress js-scrolltop" aria-label="Scroll to top">
                    <span class="icon-arrow-top" aria-hidden="true"></span>
                    <svg aria-hidden="true">
                      <circle class="progress-ring__circle js-progress" fill="transparent" r="0"></circle>
                    </svg>
                  </button>
                </div>
              </div>
              <figure class="kg-card kg-image-card kg-width-wide"><img src="https://cdn-images-1.medium.com/max/1200/0*E1pzpUrIPoMfaWNg.jpg" class="kg-image" alt loading="lazy"></figure><p>We all have noticed how capturing images with the sun (or any bright objects) in the background usually doesn’t turn out well. The image comes out to be either too dark or too bright depending on the focus. Let’s try to understand why this happens and how this can be solved.</p><p>There are a lot of key concepts that revolve around the study of HDR images -</p><ul><li><strong>Dynamic range</strong></li><li><strong>Image exposure</strong></li><li><strong>Shutter speed, Aperture, ISO</strong></li><li><strong>Image bracketing</strong></li><li><strong>Merging LDR images</strong></li><li><strong>Image encoding</strong></li><li><strong>Camera response function</strong></li><li><strong>Linearisation</strong></li><li><strong>Gamma correction</strong></li><li><strong>Tonemapping</strong></li><li><strong>Visualising HDR images</strong></li></ul><h3 id="dynamic-range">Dynamic Range</h3><p>Dynamic range of a scene refers to the range of light intensity that encompasses a scene. It can also be defined as the ratio of light (maximum measurable brightness) to dark (minimum measurable brightness) in an image.</p><p>To get some context of the how brightness can be quantised, the range of light intensity is 0 to infinity, with zero being the darkest and infinity being the brightest source there is (☀️) .</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*YX1gBta3mldqWhvBMUmqsA.png" class="kg-image" alt loading="lazy"><figcaption>Luminance value comparison (<a href="https://www.hdrsoft.com/resources/dri.html" rel="noopener">Source</a>)</figcaption></figure><p>No camera is able to capture this complete uncapped range of illuminance in a scene. Therefore, images turn out to be either too bright (overexposed) or too dark (underexposed). These images are called <em>Low Dynamic Range</em> (LDR) images . For images that turn out too bright, it is only the brighter subrange (of the infinite range) that the camera is able to capture, and correspondingly for the darker images, only the lower subrange is captured.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*CzMITGf7VZ6ly2wLKz_d1g.png" class="kg-image" alt loading="lazy"><figcaption><strong>True image</strong> (Left), <strong>Overexposed image</strong> (Center), <strong>Underexposed image</strong> (Right)</figcaption></figure><h3 id="image-exposure">Image Exposure</h3><p>The amount of light entering the camera (and thus, the image) is called the exposure. The exposure of the image can be controlled by three settings of a camera — the aperture, shutter speed and ISO.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*mQ_fmmAzRkmKX63SrfCV8A.png" class="kg-image" alt loading="lazy"></figure><p><strong>Aperture:</strong> The area of the camera lens through which the light can come in.</p><p><strong>Shutter speed:</strong> The speed with which the shutter of the camera closes. As the shutter speed increases, the amount of light entering the camera decreases, and vice versa. It also improves the sharpness of an image.</p><p><strong>ISO: </strong>Sensitivity of the camera sensor to incoming light.</p><p>I found this nice analogy <a href="https://www.cambridgeincolour.com/tutorials/camera-exposure.htm" rel="noopener">here</a>, between the camera settings and a bucket left out in the rain.</p><blockquote>In photography, the exposure settings of aperture, shutter speed and ISO speed are analogous to the width, time and quantity in the discussion above. Furthermore, just as the rate of rainfall was beyond your control above, so too is natural light for a photographer.</blockquote><p>Back to <em>dynamic range</em>. A single image captured from the camera can not contain the wide range of light intensity.</p><p>This problem can be solved by merging images captured at multiple exposure values. How this helps is that the overexposed images work well for the darker regions in the image, and the underexposed images are able to tone down the intensity in the extra-bright regions. Different regions of the image are captured better at different exposure values. Therefore, the idea is to merge these set of images and to recover an image with a <em>high dynamic range</em> (HDR).</p><h3 id="image-bracketing">Image bracketing</h3><p>Bracketing refers to capturing multiple images of the same scene with different camera settings. It is usually done automatically by the camera. What happens when you use the HDR feature on your smart phone is that the phone captures 3 (usually) images at three different exposure times (or exposure values) in quick succession. The lower the exposure time, the lesser the amount of light that gets in. These three images are merged by the camera software and are saved as a single image, in a way that the best portions of each image make it to the final image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*gkqV3yHAhfQXEYGY.jpg" class="kg-image" alt loading="lazy"><figcaption>A collage of 5 bracketed images that I found on the <a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiR0PPA4JHjAhVQb30KHdH6DaUQjhx6BAgBEAM&amp;url=https%3A%2F%2Fwww.highdynamicranger.com%2Fhow-to-use-auto-exposure-bracketing-for-hdr%2F&amp;psig=AOvVaw0b8jjGtlbQ010BQW1YHPIQ&amp;ust=1562002928887447" rel="noopener">internet</a>.</figcaption></figure><p>The funny thing here is that the image that is saved on your phone after the merging happens, is still not (technically) an HDR image. This is where the image encodings come into the picture (and also tonemapping — which we’ll discuss later).</p><h3 id="image-encoding">Image encoding</h3><p>Commonly, the images that we see on our phones and computers, are 8-bit (per channel) encoded RGB images. Each pixel’s value is stored using 24-bit representations, 8-bit for each channel (R, G, B). Each channel of a pixel has a range of 0–255 intensity values.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*KyT2vpk6-OCt_Nu9SXCNyw.png" class="kg-image" alt loading="lazy"><figcaption>Example of 24-bit (3x8-bit) encoding for an RGB pixel</figcaption></figure><p>The problem with this encoding that it is not capable of containing the large dynamic range of natural scenes. It only allows a range of 0–255 (only integers) for accommodating the intensity range, which is not sufficient.</p><p>To solve this problem, HDR images are encoded using 32-bit floating point numbers, for each channel. This allows us to capture the wide uncapped range of HDR images. There are various formats for writing HDR images, the most common being .hdr and .exr. All HDR images are 32-bit encodings but not all 32-bit images can be HDR images.</p><h3 id="camera-response-function-crf-">Camera Response Function (CRF)</h3><p>CRF is a function that shows the relationship between the actual scene irradiance and the digital brightness values in the image. It is also called as the Opto-electrical transfer function. Camera companies don’t provide their CRFs and consider it as proprietary information.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*XA43PkIl_VE44Asilb5z2w.png" class="kg-image" alt loading="lazy"><figcaption>Camera response function example</figcaption></figure><p>In an ideal world, the CRF should have a linear graph — meaning, the brightness value of the pixels in the image should be directly proportional to the actual irradiance in the scene. This is true for HDR images, but not for the usual images where the brightness values are altered to be able to contain them in a finite range. The more important reason for conventional images being de-linearised depends on how display devices work.</p><p>Back in the time of CRT (Cathode Ray Tube) displays, electrons were fired on a phosphor surface. The phosphor screen is known to emit photons upon being hit by accelerated electrons. However the brightness of the display didn’t vary linearly with the strength of the electron beam. This problem has been solved by modifying the incoming image/video source signals non-linearly in the direction opposite to the display’s non-linearity. By doing this, we can get a fair linear estimate of the natural scene brightness.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*UW_uGegQxkl17PlHU0f2WA.png" class="kg-image" alt loading="lazy"></figure><p>This de-linearisation of the source allows to compensate for a non-linear display. Display technologies have advanced but the non-linearity still exists in most devices. This <em>de-linearisation</em> is known as gamma correction.</p><blockquote>Gamma corrected image = image ^ γ</blockquote><p>If the input image is x, then what a display device of gamma=1.2 shows is x^(1.2). Therefore, the input image is encoded as x^(1/1.2) so that the monitor converts it to x^((1/1.2) x 1.2) which is equal to x, the original image captured by the camera.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*4k6-goBZ-RVLWjd9psEXAA.png" class="kg-image" alt loading="lazy"></figure><p>For most displays these days, images have to be encoded by a gamma value of 0.45 (1/2.2) because of a gamma decoding of 2.2 by the displays.</p><p>Gamma encoding is performed over a range of [0,1]. So images first have to be normalised by dividing by 255 and then again multiplying by 255 after the gamma operation. Powers of greater than 1 yield darker images whereas powers of less than 1 yield brighter image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*grxUUm_ZH2CmaLs-ynH0EA.png" class="kg-image" alt loading="lazy"><figcaption>Comparison between gamma encoded images</figcaption></figure><p>HDR photography (or any photography) is quite complex in a way that we need to think of three important aspects —</p><ul><li>How the actual scene is (ground truth / uncapped dynamic range)</li><li>How the camera captures (bracketing and then merging)</li><li>How it is displayed (tonemapping)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/0*wNoLL8Iz5IIR-pEb.png" class="kg-image" alt loading="lazy"><figcaption><a href="https://www.cambridgeincolour.com/tutorials/dynamic-range.htm" data-href="https://www.cambridgeincolour.com/tutorials/dynamic-range.htm" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Source</a></figcaption></figure><p>In the context of HDR imaging, we have discussed the first two points. Now let’s look at how HDR images can be displayed.</p><h3 id="tonemapping">Tonemapping</h3><p>Most off the shelf display devices are incapable of delivering the wide uncapped range of HDR images. They expect the input source to be in the three-channel 24-bit (3x8) RGB format. Due to this reason, the wide dynamic range needs to be toned down to be able to accommodate it in the 0–255 range of RGB format. This can be done in several ways, some of which are-</p><ul><li>Conventional linear normalisation: This is the most basic way of bringing down the wide range of an HDR image.</li></ul><blockquote>tonemapped image = (img — img.min()/img.max() — img.min()) x 255</blockquote><ul><li>Reinhard tonemapping: This is one of the most commonly used tonemapping algorithm that was shared in this <a href="http://www.cmap.polytechnique.fr/~peyre/cours/x2005signal/hdr_photographic.pdf" rel="noopener">paper</a>.</li></ul><blockquote>tonemapped image = img/(1+img) x 255</blockquote><ul><li>Drago tonemapping: This tonemapper is a perception-based one that compresses the dynamic range using logarithmic functions “computed using different bases from the scene content”. The paper for this can be found <a href="http://resources.mpi-inf.mpg.de/tmo/logmap/logmap.pdf" rel="noopener">here</a>.</li></ul><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/800/1*0gbgFTFg_mlZRqdWa7b8qw.png" class="kg-image" alt loading="lazy"></figure><p>You asked for it</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/800/1*9Ss_5MkHidaiJcIU67vHUg.png" class="kg-image" alt loading="lazy"><figcaption>Results</figcaption></figure><h3 id="ongoing-research-in-generating-hdr-content">Ongoing research in generating HDR content</h3><p>The conventional approach of generating HDR content is by merging multiple images captured at different exposures (bracketing). However, this approach is likely to create ghosting (blur) artifacts when there is movement between the frames. This has been solved by first aligning the neighbouring frame with the reference frame (middle frame) using something known as <em>optical flow</em>. That can be a topic for another blog post but for now we can think of it as a way to estimate the motion of objects (or pixels) that happens across frames by assigning a displacement vector to certain pixel positions.</p><p>There also has been work in generating HDR frames from singular LDR counterparts using Deep Learning. Neural networks are successfuly able to learn complex representations between the input and the output, and have thus performed quite well in learning the LDR to HDR mapping. These are some of the state-of-the-art methods for HDR image generation from a single image —</p><ul><li>HDRCNN: <a href="http://hdrv.org/hdrcnn/material/sga17_paper.pdf" rel="noopener">HDR image reconstruction from a single exposure using deep CNNs</a></li><li><a href="https://arxiv.org/pdf/1903.01277.pdf" rel="noopener">Deep Inverse Tone Mapping Using LDR Based Learning for Estimating HDR Images with Absolute Luminance</a></li><li><a href="https://arxiv.org/abs/1803.02266" rel="noopener">ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content</a></li></ul><p>Here are some state-of-the-art Deep Learning based methods for HDR image generation using multiple LDR images —</p><ul><li><a href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener">Deep High Dynamic Range Imaging of Dynamic Scenes</a></li><li>AHDRNet: <a href="https://arxiv.org/abs/1904.10293" rel="noopener">Attention-guided Network for Ghost-free High Dynamic Range Imaging</a></li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://towardsdatascience.com/paper-review-attention-guided-network-for-ghost-free-high-dynamic-range-imaging-4df2ec378e8"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Paper review — Attention-guided Network for Ghost-free High Dynamic Range Imaging</div><div class="kg-bookmark-description">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet) is the current state-of-the-art in HDR image generation using bracketed exposure images. It was presented at CVPR 2019…</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><span class="kg-bookmark-author">Towards Data Science</span><span class="kg-bookmark-publisher">Mukul Khanna</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://miro.medium.com/max/1200/0*XQTvMhuOvww-TJ8X.jpg"></div></a></figure><h3 id="how-to-view-hdr-images">How to view HDR images</h3><p>HDR images are stored as luminance maps and not as conventional RGB images and thus can’t be viewed using common image viewing applications.</p><p>MacOS allows you to view .hdr and .exr files using the Preview and Finder app. You can also use the <a href="https://viewer.openhdr.org/" rel="noopener">OpenHDR</a> website to visualise such images.</p><hr><p>Thanks for reading.</p><p>For future blog posts, I would like to discuss about how operations can be performed on HDR images using Python, OpenCV and Numpy. I would also like to share the current scenario of research being done for HDR video generation.</p>
            </div>
          </div>
        </div>
        <section class="m-recommended">
          <div class="l-wrapper in-recommended">
            <h3 class="m-section-title in-recommended"><b>Recommended for you</b></h3>
            <div class="m-recommended-articles">
              <div class="m-recommended-slider glide js-recommended-slider">
                <div class="glide__track" data-glide-el="track">
                  <div class="glide__slides">
                    
    <div class="m-recommended-slider__item glide__slide">
  <article class="m-article-card  post tag-computer-vision tag-machine-learning tag-human-computer-interaction tag-podcast">
    <div class="m-article-card__picture">
      <a href="../dnn-9-computer-vision-machine-learning-inside-the-car/index.html" class="m-article-card__picture-link" aria-hidden="true" tabindex="-1"></a>
        <img class="m-article-card__picture-background" src="../content/images/size/w600/2020/10/Podcast-thumbnails-2.009.jpeg" loading="lazy" alt="">
    </div>
      <div class="m-article-card__info">
        <a href="../tag/computer-vision/index.html" class="m-article-card__tag">Computer Vision</a>
      <a href="../dnn-9-computer-vision-machine-learning-inside-the-car/index.html" class="m-article-card__info-link" aria-label="DNN 9: NVIDIA's AI Co-Pilot , Computer Vision &amp; ML Inside The Car // Shalini De Mello">
        <div>
          <h2 class="m-article-card__title js-article-card-title " title="DNN 9: NVIDIA's AI Co-Pilot , Computer Vision &amp; ML Inside The Car // Shalini De Mello">
            DNN 9: NVIDIA's AI Co-Pilot , Computer Vision &amp; ML Inside The Car // Shalini De Mello
          </h2>
        </div>
        <div class="m-article-card__timestamp">
          <span>10 months ago</span>
          <span>•</span>
          <span>1 min read</span>
        </div>
      </a>
    </div>
  </article>
    </div>
    <div class="m-recommended-slider__item glide__slide">
  <article class="m-article-card  post tag-computer-vision tag-deep-learning tag-machine-learning tag-podcast">
    <div class="m-article-card__picture">
      <a href="../dnn-8/index.html" class="m-article-card__picture-link" aria-hidden="true" tabindex="-1"></a>
        <img class="m-article-card__picture-background" src="../content/images/size/w600/2020/10/Podcast-thumbnails-2.008.jpeg" loading="lazy" alt="">
    </div>
      <div class="m-article-card__info">
        <a href="../tag/computer-vision/index.html" class="m-article-card__tag">Computer Vision</a>
      <a href="../dnn-8/index.html" class="m-article-card__info-link" aria-label="DNN 8: Computer Vision &amp; ML Research, Super SloMo // Varun Jampani">
        <div>
          <h2 class="m-article-card__title js-article-card-title " title="DNN 8: Computer Vision &amp; ML Research, Super SloMo // Varun Jampani">
            DNN 8: Computer Vision &amp; ML Research, Super SloMo // Varun Jampani
          </h2>
        </div>
        <div class="m-article-card__timestamp">
          <span>a year ago</span>
          <span>•</span>
          <span>1 min read</span>
        </div>
      </a>
    </div>
  </article>
    </div>
    <div class="m-recommended-slider__item glide__slide">
  <article class="m-article-card  post tag-8k-views tag-paper-review tag-computer-vision tag-deep-learning">
    <div class="m-article-card__picture">
      <a href="../densenet-densely-connected-convolutional-networks/index.html" class="m-article-card__picture-link" aria-hidden="true" tabindex="-1"></a>
        <img class="m-article-card__picture-background" src="../content/images/size/w600/2020/10/1_TeHVqikNc68QC98Vm9M98Q.gif" loading="lazy" alt="">
    </div>
      <div class="m-article-card__info">
        <a href="../tag/8k-views/index.html" class="m-article-card__tag">8k+ views</a>
      <a href="../densenet-densely-connected-convolutional-networks/index.html" class="m-article-card__info-link" aria-label="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑">
        <div>
          <h2 class="m-article-card__title js-article-card-title " title="DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑">
            DenseNet -Densely Connected Convolutional Networks (CVPR 2017) 📑
          </h2>
        </div>
        <div class="m-article-card__timestamp">
          <span>a year ago</span>
          <span>•</span>
          <span>7 min read</span>
        </div>
      </a>
    </div>
  </article>
    </div>
                  </div>
                </div>
                <div data-glide-el="controls" class="glide__arrows js-controls">
                  <button data-glide-dir="&lt;" class="m-icon-button filled in-recommended-articles glide-prev" aria-label="Previous">
                    <span class="icon-arrow-left" aria-hidden="true"></span>
                  </button>
                  <button data-glide-dir="&gt;" class="m-icon-button filled in-recommended-articles glide-next" aria-label="Next">
                    <span class="icon-arrow-right" aria-hidden="true"></span>
                  </button>
                </div>
              </div>
            </div>
          </div>
        </section>
    </div>
  </article>
</main>



    
<div class="m-search js-search" role="dialog" aria-modal="true" aria-label="Search">
  <button class="m-icon-button outlined as-close-search js-close-search" aria-label="Close search">
    <span class="icon-close" aria-hidden="true"></span>
  </button>
  <div class="m-search__content">
    <form class="m-search__form">
      <div class="pos-relative">
        <span class="icon-search m-search-icon" aria-hidden="true"></span>
        <label for="search-input" class="sr-only">
          Type to search
        </label>
        <input id="search-input" type="text" class="m-input in-search js-input-search" placeholder="Type to search">
      </div>
    </form>
    <div class="js-search-results hide"></div>
    <p class="m-not-found align-center hide js-no-results">
      No results for your search, please try with something else.
    </p>
  </div>
</div>

    
<footer class="m-footer">
  <div class="m-footer__content">
    <p class="m-footer-copyright">
      <span>Mukul Khanna © 2021</span>
      <span>  •  </span>
      <span>Published w/ <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a></span>
    </p>

    <nav class="m-footer-social">
        <a href="https://www.youtube.com/channel/UC66w1T4oMv66Jn1LR5CW2yg" target="_blank" rel="noopener" aria-label="Youtube">
          <span class="icon-youtube" aria-hidden="true"></span>
        </a>
        <a href="https://twitter.com/mkulkhanna" target="_blank" rel="noopener" aria-label="Twitter">
          <span class="icon-twitter" aria-hidden="true"></span>
        </a>
      <a href="http://localhost:2368/rss" aria-label="RSS">
        <span class="icon-rss" aria-hidden="true"></span>
      </a>
    </nav>
  </div>
</footer>

    <div class="m-alert success subscribe js-alert" data-notification="subscribe">
  Great! You've successfully subscribed.
  <button class="m-alert__close js-notification-close" aria-label="Close">
    <span class="icon-close"></span>
  </button>
</div>

<div class="m-alert success signup js-alert" data-notification="signup">
  Great! Next, complete checkout for full access.
  <button class="m-alert__close js-notification-close" aria-label="Close">
    <span class="icon-close"></span>
  </button>
</div>

<div class="m-alert success signin js-alert" data-notification="signin">
  Welcome back! You've successfully signed in.
  <button class="m-alert__close js-notification-close" aria-label="Close">
    <span class="icon-close"></span>
  </button>
</div>

<div class="m-alert success checkout js-alert" data-notification="checkout">
  Success! Your account is fully activated, you now have access to all content.
  <button class="m-alert__close js-notification-close" aria-label="Close">
    <span class="icon-close"></span>
  </button>
</div>
    <script crossorigin="anonymous" src="https://polyfill.io/v3/polyfill.min.js?features=IntersectionObserver%2CPromise%2CArray.prototype.includes%2CString.prototype.endsWith%2CString.prototype.startsWith%2CObject.assign%2CNodeList.prototype.forEach"></script>
    <script defer src="../assets/js/manifest.js?v=56642d80f4"></script>
    <script defer src="../assets/js/vendor/content-api.min.js?v=56642d80f4"></script>
    <script defer src="../assets/js/vendor.js?v=56642d80f4"></script>
    <script defer src="../assets/js/app.js?v=56642d80f4"></script>

      <script defer src="../assets/js/post.js?v=56642d80f4"></script>


    <!-- prism.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-markup.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-css.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-javascript.min.js"></script>
  </body>
</html>